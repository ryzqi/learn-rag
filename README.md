# RAGæŠ€æœ¯å…¨é¢å­¦ä¹ æŒ‡å—

> æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†16ç§ä¸åŒçš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å®ç°æ–¹æ³•ï¼ŒåŒ…æ‹¬æ ¸å¿ƒåŸç†ã€æµç¨‹å›¾ã€å…³é”®ä»£ç å’Œå®è·µå»ºè®®ã€‚

## ğŸ“š ç›®å½•

- [ç¬¬ä¸€éƒ¨åˆ†ï¼šRAGåŸºç¡€æ¦‚å¿µ](#ç¬¬ä¸€éƒ¨åˆ†ragåŸºç¡€æ¦‚å¿µ)
- [ç¬¬äºŒéƒ¨åˆ†ï¼š16ç§RAGæ–¹æ³•è¯¦è§£](#ç¬¬äºŒéƒ¨åˆ†16ç§ragæ–¹æ³•è¯¦è§£)
  - [1. SimpleRAG - åŸºç¡€RAGå®ç°](#1-simplerag---åŸºç¡€ragå®ç°)
  - [2. AdaptiveRAG - è‡ªé€‚åº”æŸ¥è¯¢åˆ†ç±»RAG](#2-adaptiverag---è‡ªé€‚åº”æŸ¥è¯¢åˆ†ç±»rag)
  - [3. HyDERAG - å‡è®¾æ–‡æ¡£åµŒå…¥RAG](#3-hyderag---å‡è®¾æ–‡æ¡£åµŒå…¥rag)
  - [4. CRAG - çº æ­£æ€§RAG](#4-crag---çº æ­£æ€§rag)
  - [5. SelfRAG - è‡ªåæ€RAG](#5-selfrag---è‡ªåæ€rag)
  - [6. RerankRAG - é‡æ’åºRAG](#6-rerankrag---é‡æ’åºrag)
  - [7. FusionRAG - æ··åˆæ£€ç´¢RAG](#7-fusionrag---æ··åˆæ£€ç´¢rag)
  - [8. QueryTransformRAG - æŸ¥è¯¢è½¬æ¢RAG](#8-querytransformrag---æŸ¥è¯¢è½¬æ¢rag)
  - [9. SemanticRag - è¯­ä¹‰åˆ†å—RAG](#9-semanticrag---è¯­ä¹‰åˆ†å—rag)
  - [10. HierarchyRAG - å±‚æ¬¡åŒ–æ£€ç´¢RAG](#10-hierarchyrag---å±‚æ¬¡åŒ–æ£€ç´¢rag)
  - [11. ContextualCompressionRAG - ä¸Šä¸‹æ–‡å‹ç¼©RAG](#11-contextualcompressionrag---ä¸Šä¸‹æ–‡å‹ç¼©rag)
  - [12. ContextEnrichedRAG - ä¸Šä¸‹æ–‡å¢å¼ºRAG](#12-contextenrichedrag---ä¸Šä¸‹æ–‡å¢å¼ºrag)
  - [13. ContextualChunkProcessor - ä¸Šä¸‹æ–‡æ ‡é¢˜RAG](#13-contextualchunkprocessor---ä¸Šä¸‹æ–‡æ ‡é¢˜rag)
  - [14. DocumentAugmentationRAG - æ–‡æ¡£å¢å¼ºRAG](#14-documentaugmentationrag---æ–‡æ¡£å¢å¼ºrag)
  - [15. FeedbackLoopRAG - åé¦ˆå¾ªç¯RAG](#15-feedbacklooprag---åé¦ˆå¾ªç¯rag)
  - [16. RSERAG - ç›¸å…³æ®µè½æå–RAG](#16-rserag---ç›¸å…³æ®µè½æå–rag)
- [ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯¹æ¯”åˆ†æä¸é€‰æ‹©æŒ‡å—](#ç¬¬ä¸‰éƒ¨åˆ†å¯¹æ¯”åˆ†æä¸é€‰æ‹©æŒ‡å—)
- [ç¬¬å››éƒ¨åˆ†ï¼šå®è·µå»ºè®®](#ç¬¬å››éƒ¨åˆ†å®è·µå»ºè®®)

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šRAGåŸºç¡€æ¦‚å¿µ

### ä»€ä¹ˆæ˜¯RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œè¯­è¨€ç”Ÿæˆçš„æŠ€æœ¯æ¶æ„ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆå›ç­”å‰å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†ï¼Œæä¾›æ›´å‡†ç¡®ã€æ›´æœ‰ä¾æ®çš„å›ç­”ã€‚

### RAGæ ¸å¿ƒç»„ä»¶

```mermaid
flowchart TB
    Doc["ğŸ“„ æ–‡æ¡£è¾“å…¥"] --> Process["ğŸ“ æ–‡æ¡£å¤„ç†<br/>åˆ†å—/æ¸…æ´—"]
    Process --> Embed["ğŸ”¢ å‘é‡åŒ–<br/>Embeddingç”Ÿæˆ"]
    Embed --> Store["ğŸ’¾ å‘é‡å­˜å‚¨<br/>Milvus/å‘é‡æ•°æ®åº“"]

    Query["â“ ç”¨æˆ·æŸ¥è¯¢"] --> QEmbed["ğŸ”¢ æŸ¥è¯¢å‘é‡åŒ–"]
    QEmbed --> Retrieve["ğŸ” æ£€ç´¢<br/>ç›¸ä¼¼åº¦æœç´¢"]
    Store --> Retrieve

    Retrieve --> Context["ğŸ“‹ ä¸Šä¸‹æ–‡æ„å»º"]
    Context --> LLM["ğŸ¤– LLMç”Ÿæˆ"]
    Query --> LLM
    LLM --> Answer["âœ… æœ€ç»ˆå›ç­”"]

    style Doc fill:#E3F2FD
    style Query fill:#E3F2FD
    style Process fill:#E8F5E9
    style Embed fill:#E8F5E9
    style QEmbed fill:#E8F5E9
    style Store fill:#FFF9C4
    style Retrieve fill:#F3E5F5
    style Context fill:#E8F5E9
    style LLM fill:#F3E5F5
    style Answer fill:#FFE0B2
```

### åŸºç¡€æ¶æ„è¯´æ˜

1. **æ–‡æ¡£å¤„ç†é˜¶æ®µ**ï¼šå°†åŸå§‹æ–‡æ¡£åˆ†å‰²æˆåˆé€‚å¤§å°çš„æ–‡æœ¬å—ï¼ˆchunksï¼‰
2. **å‘é‡åŒ–é˜¶æ®µ**ï¼šä½¿ç”¨Embeddingæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡
3. **å­˜å‚¨é˜¶æ®µ**ï¼šå°†å‘é‡å’ŒåŸæ–‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆå¦‚Milvusï¼‰
4. **æ£€ç´¢é˜¶æ®µ**ï¼šå°†ç”¨æˆ·æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡ï¼Œæœç´¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—
5. **ç”Ÿæˆé˜¶æ®µ**ï¼šå°†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢ä¸€èµ·è¾“å…¥LLMç”Ÿæˆç­”æ¡ˆ

### å…³é”®æŠ€æœ¯ç‚¹

- **åˆ†å—ç­–ç•¥**ï¼šå›ºå®šå¤§å°ã€é‡å åˆ†å—ã€è¯­ä¹‰åˆ†å—
- **å‘é‡æ¨¡å‹**ï¼štext-embedding-ada-002ã€BERTã€sentence-transformers
- **ç›¸ä¼¼åº¦åº¦é‡**ï¼šä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§æ°è·ç¦»ã€ç‚¹ç§¯
- **æ£€ç´¢ç­–ç•¥**ï¼šTop-Kæ£€ç´¢ã€æ··åˆæ£€ç´¢ã€é‡æ’åº
- **ä¸Šä¸‹æ–‡ä¼˜åŒ–**ï¼šå‹ç¼©ã€æ‰©å±•ã€è¿‡æ»¤

---

## ç¬¬äºŒéƒ¨åˆ†ï¼š16ç§RAGæ–¹æ³•è¯¦è§£

### 1. SimpleRAG - åŸºç¡€RAGå®ç°

#### ğŸ“– æ–¹æ³•ç®€ä»‹

SimpleRAGæ˜¯æœ€åŸºç¡€çš„RAGå®ç°ï¼Œé‡‡ç”¨æ ‡å‡†çš„"åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨ â†’ æ£€ç´¢ â†’ ç”Ÿæˆ"æµç¨‹ã€‚å®ƒä½¿ç”¨å›ºå®šå¤§å°çš„æ–‡æœ¬åˆ†å—å’Œé‡å ç­–ç•¥ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æœ¬å—ï¼Œç„¶åå°†è¿™äº›æ–‡æœ¬å—ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥åˆ°LLMä¸­ç”Ÿæˆç­”æ¡ˆã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **ç®€å•ç›´æ¥**ï¼šéµå¾ªæœ€åŸºæœ¬çš„RAGæµç¨‹ï¼Œæ²¡æœ‰å¤æ‚çš„ä¼˜åŒ–ç­–ç•¥
- **å›ºå®šåˆ†å—**ï¼šä½¿ç”¨é¢„å®šä¹‰çš„chunk_sizeå’Œoverlapå‚æ•°
- **å‘é‡æ£€ç´¢**ï¼šåŸºäºCOSINEç›¸ä¼¼åº¦çš„è¯­ä¹‰æœç´¢
- **ç›´æ¥ç”Ÿæˆ**ï¼šå°†æ£€ç´¢ç»“æœç›´æ¥ä½œä¸ºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    Start([å¼€å§‹]) --> ReadDoc[è¯»å–æ–‡æ¡£<br/>read_file]
    ReadDoc --> ChunkText[æ–‡æœ¬åˆ†å—<br/>chunk_size=1000<br/>overlap=200]

    ChunkText --> BatchEmbed[æ‰¹é‡ç”ŸæˆEmbedding<br/>batch_size=64]
    BatchEmbed --> StoreVec[å­˜å‚¨åˆ°Milvus<br/>id+vector+text+metadata]

    StoreVec --> WaitQuery{ç­‰å¾…æŸ¥è¯¢}
    WaitQuery --> QueryInput[ç”¨æˆ·è¾“å…¥æŸ¥è¯¢]

    QueryInput --> QueryEmbed[æŸ¥è¯¢å‘é‡åŒ–]
    QueryEmbed --> Search[Milvuså‘é‡æ£€ç´¢<br/>COSINEç›¸ä¼¼åº¦<br/>limit=5]

    Search --> BuildContext[æ„å»ºä¸Šä¸‹æ–‡<br/>æ‹¼æ¥æ£€ç´¢åˆ°çš„æ–‡æœ¬å—]
    BuildContext --> GenPrompt[ç”Ÿæˆæç¤ºè¯<br/>ä¸Šä¸‹æ–‡ + ç”¨æˆ·é—®é¢˜]

    GenPrompt --> LLM[LLMç”Ÿæˆå›ç­”<br/>Gemini]
    LLM --> Response[è¿”å›ç­”æ¡ˆ]
    Response --> End([ç»“æŸ])

    style Start fill:#E3F2FD
    style ReadDoc fill:#E8F5E9
    style ChunkText fill:#E8F5E9
    style BatchEmbed fill:#E8F5E9
    style StoreVec fill:#FFF9C4
    style QueryInput fill:#E3F2FD
    style QueryEmbed fill:#E8F5E9
    style Search fill:#F3E5F5
    style BuildContext fill:#E8F5E9
    style GenPrompt fill:#E8F5E9
    style LLM fill:#F3E5F5
    style Response fill:#FFE0B2
    style End fill:#FFE0B2
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def chunk_text(self, text: str) -> List[str]:
    """æ–‡æœ¬åˆ†å—ï¼šä½¿ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥"""
    chunks = []
    step = self.chunk_size - self.overlap  # è®¡ç®—æ­¥é•¿

    for i in range(0, len(text), step):
        chunk = text[i:i + self.chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    return chunks

def process_document(self, file_path: str) -> Dict[str, Any]:
    """å¤„ç†æ–‡æ¡£å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
    # 1. è¯»å–æ–‡æ¡£
    text = self.file_reader.read_file(file_path)

    # 2. åˆ†å—å¤„ç†
    text_chunks = self.chunk_text(text)

    # 3. æ‰¹é‡ç”Ÿæˆembedding
    embeddings = self._batch_embed_texts(text_chunks)

    # 4. å‡†å¤‡æ•°æ®å¹¶å­˜å‚¨åˆ°Milvus
    data_to_insert = []
    for i, (chunk, embedding) in enumerate(zip(text_chunks, embeddings)):
        data_to_insert.append({
            "id": self._generate_chunk_id(file_path, i),
            "vector": embedding,
            "text": chunk,
            "source": file_path,
            "chunk_index": i
        })

    # 5. æ‰¹é‡æ’å…¥
    result = self.milvus_client.insert_data(
        self.collection_name, data_to_insert
    )
    return result

def query(self, question: str, limit: int = 3) -> str:
    """å®Œæ•´æŸ¥è¯¢æµç¨‹"""
    # 1. æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
    search_results = self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=question,
        limit=limit,
        output_fields=["text", "source"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n".join([
        f"ä¸Šä¸‹æ–‡{i+1}:\n{result['entity']['text']}"
        for i, result in enumerate(search_results)
    ])

    # 3. ç”Ÿæˆå›ç­”
    user_prompt = f"ä¸Šä¸‹æ–‡:\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}"
    return self.llm_client.generate_text(
        user_prompt,
        system_instruction=self.system_prompt
    )
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **æ–‡æœ¬åˆ†å—ç®—æ³•**ï¼š

   - ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼ˆSliding Windowï¼‰æ–¹æ³•
   - `step = chunk_size - overlap` ç¡®ä¿ç›¸é‚»å—æœ‰é‡å 
   - é‡å éƒ¨åˆ†å¯ä»¥ä¿ç•™ä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼Œé¿å…ä¿¡æ¯æ–­è£‚
2. **å‘é‡ç›¸ä¼¼åº¦è®¡ç®—**ï¼š

   - ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼š`cosine_similarity = dot(v1, v2) / (||v1|| * ||v2||)`
   - å€¼åŸŸï¼š[-1, 1]ï¼Œè¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šç›¸ä¼¼
   - Milvusè‡ªåŠ¨è®¡ç®—å¹¶æ’åºè¿”å›Top-Kç»“æœ
3. **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼š

   - å°†æ–‡æœ¬å—åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹64ä¸ª
   - å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œæé«˜å¤„ç†æ•ˆç‡

#### âœ… ä¼˜ç‚¹

- **ç®€å•æ˜“æ‡‚**ï¼šå®ç°é€»è¾‘æ¸…æ™°ï¼Œå®¹æ˜“ç†è§£å’Œç»´æŠ¤
- **é€šç”¨æ€§å¼º**ï¼šé€‚ç”¨äºå¤§å¤šæ•°åŸºç¡€RAGåœºæ™¯
- **æ€§èƒ½ç¨³å®š**ï¼šæ²¡æœ‰å¤æ‚é€»è¾‘ï¼Œå‡ºé”™æ¦‚ç‡ä½
- **èµ„æºé«˜æ•ˆ**ï¼šè®¡ç®—å¼€é”€å¯é¢„æµ‹ï¼Œæ˜“äºä¼˜åŒ–

#### âŒ ç¼ºç‚¹

- **å›ºå®šåˆ†å—**ï¼šæ— æ³•é€‚åº”ä¸åŒç±»å‹çš„æ–‡æœ¬ç»“æ„
- **æ— ä¼˜åŒ–ç­–ç•¥**ï¼šæ£€ç´¢ç»“æœè´¨é‡å®Œå…¨ä¾èµ–embeddingæ¨¡å‹
- **ä¸Šä¸‹æ–‡é™åˆ¶**ï¼šç®€å•æ‹¼æ¥å¯èƒ½å¯¼è‡´tokenè¶…é™
- **ç¼ºä¹åé¦ˆ**ï¼šæ— æ³•æ ¹æ®ç”¨æˆ·åé¦ˆæ”¹è¿›

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- ç®€å•é—®ç­”ç³»ç»Ÿ
- æ–‡æ¡£æ£€ç´¢åŠ©æ‰‹
- çŸ¥è¯†åº“æŸ¥è¯¢
- POCéªŒè¯å’Œå¿«é€ŸåŸå‹
- å¯¹å‡†ç¡®åº¦è¦æ±‚ä¸é«˜çš„åœºæ™¯

---

### 2. AdaptiveRAG - è‡ªé€‚åº”æŸ¥è¯¢åˆ†ç±»RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

AdaptiveRAGé€šè¿‡åˆ†ææŸ¥è¯¢ç±»å‹ï¼ŒåŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ£€ç´¢ç­–ç•¥ã€‚ç³»ç»Ÿé¦–å…ˆä½¿ç”¨LLMå¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œåˆ†ç±»ï¼ˆäº‹å®æ€§ã€åˆ†ææ€§ã€è§‚ç‚¹æ€§ã€ä¸Šä¸‹æ–‡ç›¸å…³ï¼‰ï¼Œç„¶åæ ¹æ®ä¸åŒç±»å‹é‡‡ç”¨ä¸åŒçš„æ£€ç´¢å’Œå›ç­”ç­–ç•¥ï¼Œä»è€Œæä¾›æ›´ç²¾å‡†çš„ç­”æ¡ˆã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **æŸ¥è¯¢ç†è§£**ï¼šä½¿ç”¨LLMåˆ†ææŸ¥è¯¢æ„å›¾å’Œç±»å‹
- **ç­–ç•¥é€‚é…**ï¼šä¸ºä¸åŒæŸ¥è¯¢ç±»å‹è®¾è®¡ä¸“é—¨çš„æ£€ç´¢ç­–ç•¥
- **åŠ¨æ€è°ƒæ•´**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾è°ƒæ•´æ£€ç´¢å‚æ•°
- **åˆ†ç±»ä½“ç³»**ï¼šFactualï¼ˆäº‹å®ï¼‰ã€Analyticalï¼ˆåˆ†æï¼‰ã€Opinionï¼ˆè§‚ç‚¹ï¼‰ã€Contextualï¼ˆä¸Šä¸‹æ–‡ï¼‰

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    Start([ç”¨æˆ·æŸ¥è¯¢]) --> Classify[LLMåˆ†ç±»æŸ¥è¯¢ç±»å‹]

    Classify --> Decision{"æŸ¥è¯¢ç±»å‹?"}

    Decision -->|"Factual<br/>äº‹å®æ€§æŸ¥è¯¢"| FactStrategy["äº‹å®æ£€ç´¢ç­–ç•¥<br/>â€¢ ç²¾ç¡®åŒ¹é…<br/>â€¢ limit=3<br/>â€¢ é«˜ç›¸ä¼¼åº¦é˜ˆå€¼"]
    Decision -->|"Analytical<br/>åˆ†ææ€§æŸ¥è¯¢"| AnaStrategy["åˆ†ææ£€ç´¢ç­–ç•¥<br/>â€¢ å¤šè§’åº¦æ£€ç´¢<br/>â€¢ limit=5<br/>â€¢ ç»¼åˆå¤šä¸ªæ¥æº"]
    Decision -->|"Opinion<br/>è§‚ç‚¹æ€§æŸ¥è¯¢"| OpiStrategy["è§‚ç‚¹æ£€ç´¢ç­–ç•¥<br/>â€¢ æ£€ç´¢å¤šæ ·è§‚ç‚¹<br/>â€¢ limit=5<br/>â€¢ å¹³è¡¡ä¸åŒç«‹åœº"]
    Decision -->|"Contextual<br/>ä¸Šä¸‹æ–‡æŸ¥è¯¢"| ConStrategy["ä¸Šä¸‹æ–‡æ£€ç´¢ç­–ç•¥<br/>â€¢ æ£€ç´¢ç›¸é‚»å—<br/>â€¢ limit=7<br/>â€¢ ä¿æŒè¿è´¯æ€§"]

    FactStrategy --> Execute[æ‰§è¡Œæ£€ç´¢]
    AnaStrategy --> Execute
    OpiStrategy --> Execute
    ConStrategy --> Execute

    Execute --> BuildPrompt["æ„å»ºæç¤ºè¯<br/>æ ¹æ®æŸ¥è¯¢ç±»å‹å®šåˆ¶"]
    BuildPrompt --> LLM["LLMç”Ÿæˆ<br/>ä½¿ç”¨ç±»å‹ç‰¹å®šæŒ‡ä»¤"]
    LLM --> Response([è¿”å›ç­”æ¡ˆ])

    style Start fill:#E3F2FD
    style Classify fill:#F3E5F5
    style Decision fill:#FFF9C4
    style FactStrategy fill:#E8F5E9
    style AnaStrategy fill:#E8F5E9
    style OpiStrategy fill:#E8F5E9
    style ConStrategy fill:#E8F5E9
    style BuildPrompt fill:#E8F5E9
    style LLM fill:#F3E5F5
    style Response fill:#FFE0B2
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def _classify_query(self, query: str) -> str:
    """ä½¿ç”¨LLMå¯¹æŸ¥è¯¢è¿›è¡Œåˆ†ç±»"""
    classification_prompt = """
    è¯·å°†ä»¥ä¸‹æŸ¥è¯¢åˆ†ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š
    - Factualï¼šå¯»æ±‚äº‹å®ä¿¡æ¯çš„æŸ¥è¯¢
    - Analyticalï¼šéœ€è¦åˆ†æã€æ¯”è¾ƒæˆ–æ¨ç†çš„æŸ¥è¯¢
    - Opinionï¼šå¯»æ±‚è§‚ç‚¹ã€è¯„ä»·æˆ–å»ºè®®çš„æŸ¥è¯¢
    - Contextualï¼šéœ€è¦ç‰¹å®šä¸Šä¸‹æ–‡æˆ–è¿ç»­ä¿¡æ¯çš„æŸ¥è¯¢

    åªè¿”å›ç±»å‹åç§°ï¼Œä¸è¦è§£é‡Šã€‚

    æŸ¥è¯¢ï¼š{query}
    """

    response = self.llm_client.generate_text(
        classification_prompt.format(query=query)
    )
    return response.strip()

def _factual_retrieval_strategy(self, query: str) -> List[Dict]:
    """äº‹å®æ€§æŸ¥è¯¢ç­–ç•¥ï¼šç²¾ç¡®æ£€ç´¢å°‘é‡æœ€ç›¸å…³ç»“æœ"""
    return self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=query,
        limit=3,  # äº‹å®æŸ¥è¯¢åªéœ€è¦æœ€ç›¸å…³çš„å‡ ä¸ªç»“æœ
        output_fields=["text", "source"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

def _analytical_retrieval_strategy(self, query: str) -> List[Dict]:
    """åˆ†ææ€§æŸ¥è¯¢ç­–ç•¥ï¼šæ£€ç´¢æ›´å¤šç»“æœä»¥æ”¯æŒåˆ†æ"""
    return self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=query,
        limit=5,  # åˆ†ææŸ¥è¯¢éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
        output_fields=["text", "source"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

def _contextual_retrieval_strategy(self, query: str) -> List[Dict]:
    """ä¸Šä¸‹æ–‡æŸ¥è¯¢ç­–ç•¥ï¼šæ£€ç´¢è¿ç»­çš„æ–‡æœ¬å—"""
    # 1. å…ˆæ‰¾åˆ°æœ€ç›¸å…³çš„å—
    initial_results = self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=query,
        limit=3,
        output_fields=["text", "source", "chunk_index"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

    # 2. è·å–ç›¸é‚»çš„å—ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    extended_results = []
    for result in initial_results:
        chunk_index = result['entity']['chunk_index']
        source = result['entity']['source']

        # è·å–å‰åå„1ä¸ªå—
        for offset in [-1, 0, 1]:
            adjacent_chunk = self.milvus_client.query_data(
                collection_name=self.collection_name,
                filter_expr=f"source == '{source}' && chunk_index == {chunk_index + offset}",
                output_fields=["text", "source", "chunk_index"]
            )
            extended_results.extend(adjacent_chunk)

    return extended_results

def query(self, question: str) -> str:
    """è‡ªé€‚åº”æŸ¥è¯¢ä¸»æµç¨‹"""
    # 1. åˆ†ç±»æŸ¥è¯¢ç±»å‹
    query_type = self._classify_query(question)

    # 2. æ ¹æ®ç±»å‹é€‰æ‹©ç­–ç•¥
    if query_type == "Factual":
        results = self._factual_retrieval_strategy(question)
        system_instruction = "æä¾›ç®€æ´å‡†ç¡®çš„äº‹å®æ€§å›ç­”ã€‚"
    elif query_type == "Analytical":
        results = self._analytical_retrieval_strategy(question)
        system_instruction = "æä¾›æ·±å…¥çš„åˆ†æï¼Œè€ƒè™‘å¤šä¸ªè§’åº¦ã€‚"
    elif query_type == "Opinion":
        results = self._opinion_retrieval_strategy(question)
        system_instruction = "å¹³è¡¡å‘ˆç°ä¸åŒè§‚ç‚¹ã€‚"
    else:  # Contextual
        results = self._contextual_retrieval_strategy(question)
        system_instruction = "åŸºäºå®Œæ•´ä¸Šä¸‹æ–‡æä¾›è¿è´¯çš„å›ç­”ã€‚"

    # 3. æ„å»ºä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆç­”æ¡ˆ
    context = self._build_context(results)
    return self.llm_client.generate_text(
        f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{question}",
        system_instruction=system_instruction
    )
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **æŸ¥è¯¢åˆ†ç±»ç®—æ³•**ï¼š

   - ä½¿ç”¨Few-Shot Promptingå¼•å¯¼LLMç†è§£åˆ†ç±»ä»»åŠ¡
   - å››ä¸ªç±»åˆ«åŸºäºæŸ¥è¯¢æ„å›¾å’Œä¿¡æ¯éœ€æ±‚ç‰¹å¾
   - åˆ†ç±»ç»“æœç›´æ¥å½±å“åç»­æ£€ç´¢å‚æ•°
2. **è‡ªé€‚åº”æ£€ç´¢å‚æ•°**ï¼š

   - **Factual**ï¼šlimit=3ï¼Œè¿½æ±‚ç²¾ç¡®åº¦
   - **Analytical**ï¼šlimit=5ï¼Œéœ€è¦æ›´å…¨é¢çš„ä¿¡æ¯
   - **Opinion**ï¼šlimit=5ï¼Œéœ€è¦å¤šæ ·æ€§
   - **Contextual**ï¼šlimit=7ï¼ˆ3+ç›¸é‚»å—ï¼‰ï¼Œä¿æŒè¿è´¯æ€§
3. **ä¸Šä¸‹æ–‡æ‰©å±•ç­–ç•¥**ï¼š

   - å¯¹äºContextualç±»å‹ï¼Œæ£€ç´¢ç›¸é‚»chunk_indexçš„å—
   - é€šè¿‡filter_exprå®ç°ç²¾ç¡®çš„ç›¸é‚»å—æŸ¥è¯¢

#### âœ… ä¼˜ç‚¹

- **æ™ºèƒ½é€‚é…**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹ç‚¹è‡ªåŠ¨è°ƒæ•´ç­–ç•¥
- **æå‡å‡†ç¡®åº¦**ï¼šé’ˆå¯¹æ€§æ£€ç´¢æé«˜ç­”æ¡ˆè´¨é‡
- **çµæ´»æ€§å¼º**ï¼šæ˜“äºæ·»åŠ æ–°çš„æŸ¥è¯¢ç±»å‹å’Œç­–ç•¥
- **ç”¨æˆ·ä½“éªŒå¥½**ï¼šä¸åŒç±»å‹é—®é¢˜å¾—åˆ°æ›´åˆé€‚çš„ç­”æ¡ˆ

#### âŒ ç¼ºç‚¹

- **é¢å¤–å¼€é”€**ï¼šéœ€è¦é¢å¤–çš„LLMè°ƒç”¨è¿›è¡Œåˆ†ç±»
- **åˆ†ç±»å‡†ç¡®æ€§**ï¼šLLMåˆ†ç±»å¯èƒ½å‡ºé”™ï¼Œå½±å“åç»­æµç¨‹
- **å¤æ‚åº¦é«˜**ï¼šç»´æŠ¤å¤šå¥—æ£€ç´¢ç­–ç•¥å¢åŠ ç³»ç»Ÿå¤æ‚åº¦
- **æˆæœ¬å¢åŠ **ï¼šæ›´å¤šçš„LLMè°ƒç”¨æ„å‘³ç€æ›´é«˜çš„æˆæœ¬

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- å¤šæ ·åŒ–æŸ¥è¯¢åœºæ™¯ï¼ˆäº‹å®ã€åˆ†æã€è§‚ç‚¹æ··åˆï¼‰
- æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
- æ•™è‚²è¾…å¯¼ç³»ç»Ÿ
- ä¸“ä¸šå’¨è¯¢åŠ©æ‰‹
- éœ€è¦é«˜è´¨é‡ç­”æ¡ˆçš„åœºæ™¯

---

### 3. HyDERAG - å‡è®¾æ–‡æ¡£åµŒå…¥RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

HyDEï¼ˆHypothetical Document Embeddingsï¼‰RAGé€šè¿‡è®©LLMé¦–å…ˆç”Ÿæˆå¤šä¸ªå‡è®¾æ€§çš„ç­”æ¡ˆæ–‡æ¡£ï¼Œç„¶åä½¿ç”¨è¿™äº›å‡è®¾æ–‡æ¡£è€ŒéåŸå§‹æŸ¥è¯¢è¿›è¡Œæ£€ç´¢ã€‚è¿™ç§æ–¹æ³•åŸºäºä¸€ä¸ªæ´å¯Ÿï¼šå‡è®¾ç­”æ¡ˆä¸çœŸå®ç­”æ¡ˆåœ¨è¯­ä¹‰ç©ºé—´ä¸­æ›´æ¥è¿‘ï¼Œå› æ­¤èƒ½æ£€ç´¢åˆ°æ›´ç›¸å…³çš„æ–‡æ¡£ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **å‡è®¾ç”Ÿæˆ**ï¼šLLMç”ŸæˆNä¸ªå¯èƒ½çš„ç­”æ¡ˆæ–‡æ¡£
- **å¤šæ ·æ€§æ£€ç´¢**ï¼šæ¯ä¸ªå‡è®¾æ–‡æ¡£ç‹¬ç«‹æ£€ç´¢
- **ç»“æœèšåˆ**ï¼šåˆå¹¶å»é‡æ‰€æœ‰æ£€ç´¢ç»“æœ
- **äºŒæ¬¡ç”Ÿæˆ**ï¼šåŸºäºèšåˆç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B{ç”Ÿæˆå¤šä¸ª<br/>å‡è®¾æ€§ç­”æ¡ˆ};
    B --> C(å¹¶è¡Œæ£€ç´¢);
    C --> D{èšåˆä¸é‡æ’åº};
    D --> E[LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ];
    E --> F[è¿”å›ç­”æ¡ˆ];

    style A fill:#E3F2FD
    style B fill:#F3E5F5
    style C fill:#E8F5E9
    style D fill:#FFF9C4
    style E fill:#F3E5F5
    style F fill:#FFE0B2
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def generate_hypothetical_documents(
    self, query: str, num_documents: int = 3
) -> List[str]:
    """ç”Ÿæˆå¤šä¸ªå‡è®¾æ€§ç­”æ¡ˆæ–‡æ¡£"""
    # æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚ç”Ÿæˆå¤šä¸ªä¸åŒè§’åº¦çš„ç­”æ¡ˆ
    prompt = f"""
    è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆ{num_documents}ä¸ªä¸åŒçš„å‡è®¾æ€§ç­”æ¡ˆã€‚
    æ¯ä¸ªç­”æ¡ˆåº”è¯¥ä»ä¸åŒè§’åº¦æˆ–ä½¿ç”¨ä¸åŒæ–¹å¼å›ç­”é—®é¢˜ã€‚

    é—®é¢˜ï¼š{query}

    è¯·ç”Ÿæˆ{num_documents}ä¸ªç‹¬ç«‹çš„ç­”æ¡ˆï¼Œæ¯ä¸ªç­”æ¡ˆç”¨"ç­”æ¡ˆNï¼š"å¼€å¤´ã€‚
    """

    # ä½¿ç”¨è¾ƒé«˜temperatureå¢åŠ å¤šæ ·æ€§
    response = self.llm_client.generate_text(
        prompt,
        temperature=0.8  # æé«˜éšæœºæ€§
    )

    # è§£æç”Ÿæˆçš„å‡è®¾æ–‡æ¡£
    hypothetical_docs = []
    for line in response.split('\n'):
        if line.strip().startswith('ç­”æ¡ˆ'):
            # æå–ç­”æ¡ˆå†…å®¹
            doc = line.split('ï¼š', 1)[1].strip()
            if doc:
                hypothetical_docs.append(doc)

    return hypothetical_docs[:num_documents]

def search_with_hypothetical_docs(
    self, query: str, num_hypothetical: int = 3, limit: int = 5
) -> List[Dict[str, Any]]:
    """ä½¿ç”¨å‡è®¾æ–‡æ¡£è¿›è¡Œæ£€ç´¢"""
    # 1. ç”Ÿæˆå‡è®¾æ–‡æ¡£
    hypothetical_docs = self.generate_hypothetical_documents(
        query, num_hypothetical
    )

    # 2. å¯¹æ¯ä¸ªå‡è®¾æ–‡æ¡£è¿›è¡Œæ£€ç´¢
    all_results = []
    for i, hypo_doc in enumerate(hypothetical_docs):
        # ä½¿ç”¨å‡è®¾æ–‡æ¡£ä½œä¸ºæŸ¥è¯¢
        results = self.milvus_client.search_by_text(
            collection_name=self.collection_name,
            text=hypo_doc,  # æ³¨æ„ï¼šä½¿ç”¨å‡è®¾æ–‡æ¡£è€ŒéåŸå§‹æŸ¥è¯¢
            limit=limit,
            output_fields=["text", "source", "chunk_index"],
            metric_type="COSINE",
            embedding_client=self.embedding_client
        )

        # æ ‡è®°æ¥æºäºå“ªä¸ªå‡è®¾æ–‡æ¡£
        for result in results:
            result['hypothetical_index'] = i

        all_results.extend(results)

    # 3. èšåˆå’Œå»é‡
    return self.aggregate_search_results(all_results)

def aggregate_search_results(
    self, results: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """èšåˆå¤šæ¬¡æ£€ç´¢çš„ç»“æœå¹¶å»é‡"""
    # ä½¿ç”¨chunk_idä½œä¸ºå”¯ä¸€æ ‡è¯†è¿›è¡Œå»é‡
    unique_results = {}

    for result in results:
        chunk_id = result.get('id') or result['entity'].get('id')

        if chunk_id not in unique_results:
            # é¦–æ¬¡é‡åˆ°è¯¥chunkï¼Œè®°å½•åˆ†æ•°
            unique_results[chunk_id] = {
                'result': result,
                'scores': [result.get('score', 1 - result.get('distance', 0))],
                'count': 1
            }
        else:
            # å·²å­˜åœ¨è¯¥chunkï¼Œç´¯åŠ åˆ†æ•°
            unique_results[chunk_id]['scores'].append(
                result.get('score', 1 - result.get('distance', 0))
            )
            unique_results[chunk_id]['count'] += 1

    # è®¡ç®—èšåˆåˆ†æ•°ï¼ˆå¹³å‡åˆ† + å‡ºç°æ¬¡æ•°åŠ æƒï¼‰
    aggregated = []
    for chunk_id, data in unique_results.items():
        avg_score = sum(data['scores']) / len(data['scores'])
        # å‡ºç°æ¬¡æ•°è¶Šå¤šï¼Œåˆ†æ•°ç•¥å¾®æå‡
        boosted_score = avg_score * (1 + 0.1 * (data['count'] - 1))

        result = data['result']
        result['aggregated_score'] = boosted_score
        result['appearance_count'] = data['count']
        aggregated.append(result)

    # æŒ‰èšåˆåˆ†æ•°æ’åº
    aggregated.sort(key=lambda x: x['aggregated_score'], reverse=True)
    return aggregated

def query(self, question: str, num_hypothetical: int = 3, limit: int = 5) -> str:
    """å®Œæ•´çš„HyDE RAGæµç¨‹"""
    # 1. ä½¿ç”¨å‡è®¾æ–‡æ¡£æ£€ç´¢
    search_results = self.search_with_hypothetical_docs(
        question, num_hypothetical, limit
    )

    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n".join([
        f"æ–‡æ¡£{i+1} (ç›¸å…³åº¦:{result['aggregated_score']:.3f}, å‡ºç°{result['appearance_count']}æ¬¡):\n{result['entity']['text']}"
        for i, result in enumerate(search_results[:limit])
    ])

    # 3. ä½¿ç”¨çœŸå®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    final_prompt = f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}"
    return self.llm_client.generate_text(
        final_prompt,
        system_instruction=self.system_prompt
    )
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **å‡è®¾æ–‡æ¡£ç”Ÿæˆç†è®º**ï¼š

   - æŸ¥è¯¢Qå’Œç­”æ¡ˆAåœ¨embeddingç©ºé—´ä¸­è·ç¦»è¾ƒè¿œ
   - å‡è®¾ç­”æ¡ˆHä¸çœŸå®ç­”æ¡ˆAåœ¨embeddingç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘
   - ä½¿ç”¨Hæ£€ç´¢æ¯”ä½¿ç”¨Qæ›´å®¹æ˜“æ‰¾åˆ°A
2. **å¤šæ ·æ€§å¢å¼º**ï¼š

   - ä½¿ç”¨ `temperature=0.8`å¢åŠ ç”Ÿæˆå¤šæ ·æ€§
   - ç”Ÿæˆå¤šä¸ªè§’åº¦çš„å‡è®¾ç­”æ¡ˆè¦†ç›–æ›´å¹¿çš„è¯­ä¹‰ç©ºé—´
   - æ¯ä¸ªå‡è®¾æ–‡æ¡£ç‹¬ç«‹æ£€ç´¢é¿å…bias
3. **ç»“æœèšåˆç®—æ³•**ï¼š

   ```
   aggregated_score = avg_score * (1 + 0.1 * (appearance_count - 1))
   ```

   - åŸºç¡€åˆ†æ•°ï¼šå¤šæ¬¡æ£€ç´¢çš„å¹³å‡ç›¸ä¼¼åº¦
   - é¢‘æ¬¡åŠ æƒï¼šå‡ºç°å¤šæ¬¡çš„æ–‡æ¡£ç•¥å¾®æå‡åˆ†æ•°
   - æœ€ç»ˆæ’åºï¼šç»¼åˆè€ƒè™‘ç›¸å…³æ€§å’Œç¨³å®šæ€§

#### âœ… ä¼˜ç‚¹

- **æå‡å¬å›ç‡**ï¼šå‡è®¾æ–‡æ¡£ä¸ç­”æ¡ˆè¯­ä¹‰æ›´æ¥è¿‘ï¼Œæ£€ç´¢æ›´å‡†ç¡®
- **è¦†ç›–é¢å¹¿**ï¼šå¤šä¸ªå‡è®¾æ–‡æ¡£ä»ä¸åŒè§’åº¦æ£€ç´¢
- **é²æ£’æ€§å¼º**ï¼šå³ä½¿æŸä¸ªå‡è®¾ä¸å‡†ç¡®ï¼Œå…¶ä»–å‡è®¾å¯ä»¥è¡¥å¿
- **é€‚åˆå¤æ‚æŸ¥è¯¢**ï¼šå¯¹äºæŠ½è±¡æˆ–å¤æ‚é—®é¢˜æ•ˆæœæ›´å¥½

#### âŒ ç¼ºç‚¹

- **é«˜è®¡ç®—æˆæœ¬**ï¼šéœ€è¦é¢å¤–ç”ŸæˆNä¸ªå‡è®¾æ–‡æ¡£
- **å»¶è¿Ÿå¢åŠ **ï¼šå¤šæ¬¡LLMè°ƒç”¨å’Œå¤šæ¬¡æ£€ç´¢å¢åŠ å“åº”æ—¶é—´
- **å¯èƒ½åç¦»**ï¼šå‡è®¾æ–‡æ¡£å¯èƒ½å¼•å…¥é”™è¯¯æ–¹å‘
- **èµ„æºæ¶ˆè€—å¤§**ï¼šembeddingå’Œæ£€ç´¢æ¬¡æ•°æˆå€å¢åŠ 

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- å¤æ‚æˆ–æŠ½è±¡çš„æŸ¥è¯¢
- é¢†åŸŸä¸“ä¸šé—®é¢˜
- éœ€è¦é«˜å¬å›ç‡çš„åœºæ™¯
- ç”¨æˆ·æŸ¥è¯¢è¡¨è¾¾ä¸æ¸…æ™°æ—¶
- å¯¹å“åº”æ—¶é—´è¦æ±‚ä¸è‹›åˆ»çš„åœºæ™¯

---

### 4. CRAG - çº æ­£æ€§RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

CRAGï¼ˆCorrective RAGï¼‰é€šè¿‡è¯„ä¼°æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ï¼ŒåŠ¨æ€å†³å®šæ˜¯å¦éœ€è¦å¤–éƒ¨ç½‘ç»œæœç´¢æ¥è¡¥å……æˆ–æ›¿æ¢æ£€ç´¢ç»“æœã€‚ç³»ç»Ÿä¼šè®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œæ ¹æ®é˜ˆå€¼å†³å®šé‡‡ç”¨æœ¬åœ°æ–‡æ¡£ã€ç½‘ç»œæœç´¢è¿˜æ˜¯æ··åˆç­–ç•¥ï¼Œä»è€Œæä¾›æ›´å¯é çš„ç­”æ¡ˆã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **ç›¸å…³æ€§è¯„ä¼°**ï¼šä½¿ç”¨LLMè¯„ä¼°æ¯ä¸ªæ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ï¼ˆ0-1åˆ†ï¼‰
- **åŠ¨æ€å†³ç­–**ï¼šæ ¹æ®ç›¸å…³æ€§åˆ†æ•°å†³å®šä¿¡æ¯æ¥æº
- **ä¸‰ç§ç­–ç•¥**ï¼šçº¯æœ¬åœ°ã€çº¯ç½‘ç»œã€æ··åˆæ£€ç´¢
- **çŸ¥è¯†ç²¾ç‚¼**ï¼šè¿‡æ»¤ä½è´¨é‡æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[åˆæ­¥æ£€ç´¢æœ¬åœ°çŸ¥è¯†];
    B --> C{LLMè¯„ä¼°<br/>æœ¬åœ°çŸ¥è¯†ç›¸å…³æ€§};
    C --> D{æ ¹æ®ç›¸å…³æ€§<br/>é€‰æ‹©ç­–ç•¥};
  
    D -->|é«˜| E[ç­–ç•¥1: ä»…ç”¨æœ¬åœ°çŸ¥è¯†];
    D -->|ä¸­| F[ç­–ç•¥2: æ··åˆæœ¬åœ°ä¸ç½‘ç»œ];
    D -->|ä½| G[ç­–ç•¥3: ä»…ç”¨ç½‘ç»œæœç´¢];
  
    subgraph "ç”Ÿæˆç­”æ¡ˆ"
      E --> H[æ•´åˆä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ]
      F --> H
      G --> H
    end
  
    H --> I[è¿”å›ç­”æ¡ˆ];

    style A fill:#E3F2FD
    style B fill:#E8F5E9
    style C fill:#F3E5F5
    style D fill:#FFF9C4
    style E fill:#C8E6C9
    style F fill:#B3E5FC
    style G fill:#FFECB3
    style H fill:#F3E5F5
    style I fill:#FFE0B2
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def evaluate_document_relevance(
    self, query: str, document: str
) -> float:
    """è¯„ä¼°å•ä¸ªæ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼ˆ0-1åˆ†æ•°ï¼‰"""
    eval_prompt = f"""
    è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£å¯¹å›ç­”æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

    æŸ¥è¯¢ï¼š{query}

    æ–‡æ¡£ï¼š{document[:500]}...

    è¯·ç»™å‡º0åˆ°1ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æ•°ï¼š
    - 0.0: å®Œå…¨ä¸ç›¸å…³
    - 0.3: å¼±ç›¸å…³
    - 0.5: ä¸­ç­‰ç›¸å…³
    - 0.7: é«˜åº¦ç›¸å…³
    - 1.0: å®Œå…¨ç›¸å…³

    åªè¿”å›æ•°å­—åˆ†æ•°ï¼Œä¸è¦è§£é‡Šã€‚
    """

    try:
        response = self.llm_client.generate_text(eval_prompt)
        score = float(response.strip())
        return max(0.0, min(1.0, score))  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
    except:
        return 0.5  # é»˜è®¤ä¸­ç­‰ç›¸å…³

def rewrite_search_query(self, original_query: str) -> str:
    """é‡å†™æŸ¥è¯¢ä»¥è¿›è¡Œç½‘ç»œæœç´¢"""
    rewrite_prompt = f"""
    å°†ä»¥ä¸‹æŸ¥è¯¢æ”¹å†™ä¸ºæ›´é€‚åˆç½‘ç»œæœç´¢çš„å½¢å¼ã€‚
    ä½¿å…¶æ›´ç®€æ´ã€å…³é”®è¯æ˜ç¡®ã€æ˜“äºæ£€ç´¢ã€‚

    åŸå§‹æŸ¥è¯¢ï¼š{original_query}

    æ”¹å†™åçš„æŸ¥è¯¢ï¼š
    """
    return self.llm_client.generate_text(rewrite_prompt).strip()

def perform_web_search(self, query: str, limit: int = 3) -> List[str]:
    """æ‰§è¡Œç½‘ç»œæœç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„æœç´¢APIï¼ˆå¦‚Googleã€Bingï¼‰
    # è¿™é‡Œç®€åŒ–ä¸ºè¿”å›ç¤ºä¾‹
    return [
        f"ç½‘ç»œæœç´¢ç»“æœ{i+1}ï¼Œå…³äºï¼š{query}"
        for i in range(limit)
    ]

def refine_knowledge(
    self, query: str, documents: List[str]
) -> List[str]:
    """ä»æ–‡æ¡£ä¸­ç²¾ç‚¼æå–å…³é”®çŸ¥è¯†"""
    refined = []
    for doc in documents:
        refine_prompt = f"""
        ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚
        åªä¿ç•™ç›´æ¥ç›¸å…³çš„å†…å®¹ï¼Œåˆ é™¤æ— å…³ä¿¡æ¯ã€‚

        æŸ¥è¯¢ï¼š{query}
        æ–‡æ¡£ï¼š{doc}

        å…³é”®ä¿¡æ¯ï¼š
        """

        key_info = self.llm_client.generate_text(refine_prompt)
        if key_info.strip():
            refined.append(key_info.strip())

    return refined

def crag_process(
    self, query: str,
    relevance_threshold_high: float = 0.7,
    relevance_threshold_low: float = 0.3
) -> Dict[str, Any]:
    """å®Œæ•´çš„CRAGæµç¨‹"""
    # 1. åˆå§‹æ£€ç´¢
    search_results = self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=query,
        limit=5,
        output_fields=["text"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

    # 2. è¯„ä¼°æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§
    relevance_scores = []
    documents = []
    for result in search_results:
        doc_text = result['entity']['text']
        documents.append(doc_text)
        score = self.evaluate_document_relevance(query, doc_text)
        relevance_scores.append(score)

    # 3. è®¡ç®—å¹³å‡ç›¸å…³æ€§
    avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0

    # 4. æ ¹æ®å¹³å‡ç›¸å…³æ€§å†³å®šç­–ç•¥
    if avg_relevance >= relevance_threshold_high:
        # ç­–ç•¥1ï¼šé«˜ç›¸å…³æ€§ï¼Œä½¿ç”¨æœ¬åœ°æ–‡æ¡£
        strategy = "LOCAL"
        # è¿‡æ»¤ä½åˆ†æ–‡æ¡£
        filtered_docs = [
            doc for doc, score in zip(documents, relevance_scores)
            if score >= 0.5
        ]
        knowledge_base = self.refine_knowledge(query, filtered_docs)

    elif avg_relevance <= relevance_threshold_low:
        # ç­–ç•¥2ï¼šä½ç›¸å…³æ€§ï¼Œä½¿ç”¨ç½‘ç»œæœç´¢
        strategy = "WEB"
        rewritten_query = self.rewrite_search_query(query)
        web_results = self.perform_web_search(rewritten_query)
        knowledge_base = self.refine_knowledge(query, web_results)

    else:
        # ç­–ç•¥3ï¼šä¸­ç­‰ç›¸å…³æ€§ï¼Œæ··åˆä½¿ç”¨
        strategy = "HYBRID"
        # ä¿ç•™éƒ¨åˆ†æœ¬åœ°æ–‡æ¡£
        local_docs = [
            doc for doc, score in zip(documents, relevance_scores)
            if score >= 0.4
        ]
        # è¡¥å……ç½‘ç»œæœç´¢
        rewritten_query = self.rewrite_search_query(query)
        web_results = self.perform_web_search(rewritten_query, limit=2)

        combined_docs = local_docs + web_results
        knowledge_base = self.refine_knowledge(query, combined_docs)

    # 5. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    context = "\n\n".join(knowledge_base)
    answer = self.llm_client.generate_text(
        f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {query}",
        system_instruction=self.system_prompt
    )

    return {
        "query": query,
        "strategy": strategy,
        "avg_relevance": avg_relevance,
        "knowledge_sources": len(knowledge_base),
        "answer": answer
    }
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **ç›¸å…³æ€§è¯„ä¼°ç®—æ³•**ï¼š

   - ä½¿ç”¨LLMä½œä¸ºè¯„åˆ†å™¨ï¼ˆ0-1åˆ†æ•°ï¼‰
   - è¯„ä¼°æ–‡æ¡£å†…å®¹ä¸æŸ¥è¯¢çš„è¯­ä¹‰åŒ¹é…åº¦
   - æ¯”ç®€å•çš„å‘é‡ç›¸ä¼¼åº¦æ›´æ™ºèƒ½
2. **å†³ç­–æ ‘é€»è¾‘**ï¼š

   ```
   if avg_score > 0.7:
       strategy = LOCAL  # æœ¬åœ°æ–‡æ¡£è´¨é‡é«˜
   elif avg_score < 0.3:
       strategy = WEB    # æœ¬åœ°æ–‡æ¡£ä¸ç›¸å…³ï¼Œéœ€è¦ç½‘ç»œæœç´¢
   else:
       strategy = HYBRID # éœ€è¦è¡¥å……ä¿¡æ¯
   ```
3. **çŸ¥è¯†ç²¾ç‚¼**ï¼š

   - ä½¿ç”¨LLMæå–æ–‡æ¡£ä¸­ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„éƒ¨åˆ†
   - å‡å°‘å™ªå£°ï¼Œæé«˜ä¸Šä¸‹æ–‡è´¨é‡
   - ç±»ä¼¼äºextractive summarization
4. **æŸ¥è¯¢é‡å†™**ï¼š

   - å°†å¯¹è¯å¼æŸ¥è¯¢è½¬æ¢ä¸ºæœç´¢å¼•æ“å‹å¥½æ ¼å¼
   - æå–å…³é”®è¯ï¼Œå¢å¼ºæ£€ç´¢æ•ˆæœ

#### âœ… ä¼˜ç‚¹

- **è‡ªé€‚åº”å†³ç­–**ï¼šæ ¹æ®æ£€ç´¢è´¨é‡åŠ¨æ€è°ƒæ•´ç­–ç•¥
- **ç»“æœå¯é **ï¼šä½è´¨é‡æ£€ç´¢æ—¶è‡ªåŠ¨å¯»æ‰¾å¤–éƒ¨ä¿¡æ¯
- **çŸ¥è¯†ç²¾ç‚¼**ï¼šè¿‡æ»¤å™ªå£°ï¼Œæé«˜ä¸Šä¸‹æ–‡è´¨é‡
- **é€æ˜åº¦é«˜**ï¼šæ˜ç¡®æ ‡è¯†ä½¿ç”¨çš„ç­–ç•¥å’Œä¿¡æ¯æ¥æº

#### âŒ ç¼ºç‚¹

- **æˆæœ¬é«˜æ˜‚**ï¼šæ¯ä¸ªæ–‡æ¡£éƒ½éœ€è¦LLMè¯„ä¼°
- **ä¾èµ–å¤–éƒ¨**ï¼šç½‘ç»œæœç´¢éœ€è¦é¢å¤–APIå’Œæˆæœ¬
- **å»¶è¿Ÿå¢åŠ **ï¼šå¤šæ¬¡LLMè°ƒç”¨å’Œå¯èƒ½çš„ç½‘ç»œæœç´¢
- **å¤æ‚åº¦é«˜**ï¼šéœ€è¦ç»´æŠ¤å¤šä¸ªæ•°æ®æºå’Œç­–ç•¥

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- çŸ¥è¯†åº“å¯èƒ½ä¸å®Œæ•´çš„åœºæ™¯
- éœ€è¦æœ€æ–°ä¿¡æ¯çš„åº”ç”¨ï¼ˆæ–°é—»ã€æ—¶äº‹ï¼‰
- é«˜å‡†ç¡®åº¦è¦æ±‚çš„ä¸“ä¸šé¢†åŸŸ
- æ··åˆå†…å¤–éƒ¨çŸ¥è¯†æºçš„ç³»ç»Ÿ
- å¯¹é”™è¯¯å®¹å¿åº¦ä½çš„åœºæ™¯

---

### 5. SelfRAG - è‡ªåæ€RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

SelfRAGé€šè¿‡åœ¨æ£€ç´¢å’Œç”Ÿæˆçš„æ¯ä¸ªå…³é”®æ­¥éª¤è¿›è¡Œè‡ªæˆ‘åæ€å’Œè¯„ä¼°ï¼Œç¡®ä¿ç­”æ¡ˆçš„è´¨é‡ã€‚ç³»ç»Ÿä¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ã€è¯„ä¼°æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ã€éªŒè¯ç­”æ¡ˆæ˜¯å¦æœ‰æ–‡æ¡£æ”¯æŒï¼Œä»¥åŠè¯„ä¼°ç­”æ¡ˆçš„å®ç”¨æ€§ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„è‡ªæˆ‘çº æ­£å¾ªç¯ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **æ£€ç´¢å¿…è¦æ€§åˆ¤æ–­**ï¼šå…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢å¤–éƒ¨çŸ¥è¯†
- **ç›¸å…³æ€§è¯„ä¼°**ï¼šè¯„ä¼°æ£€ç´¢æ–‡æ¡£æ˜¯å¦çœŸæ­£ç›¸å…³
- **æ”¯æŒåº¦éªŒè¯**ï¼šæ£€æŸ¥ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦æœ‰æ–‡æ¡£æ”¯æŒ
- **æ•ˆç”¨è¯„åˆ†**ï¼šè¯„ä¼°ç­”æ¡ˆå¯¹ç”¨æˆ·çš„å®ç”¨ç¨‹åº¦
- **å››é‡è¯„ä¼°**ï¼šå¤šå±‚æ¬¡quality gateç¡®ä¿è´¨é‡

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B{ç¬¬ä¸€å…³: æ£€ç´¢å¿…è¦æ€§};
    B -- æ˜¯ --> C[æ£€ç´¢å¹¶æ ¡éªŒä¸Šä¸‹æ–‡];
    B -- å¦ --> G[ç›´æ¥ç”Ÿæˆ];
  
    C --> D{ç¬¬äºŒå…³: ä¸Šä¸‹æ–‡ç›¸å…³æ€§};
    D -- ç›¸å…³ --> F[ç”Ÿæˆç­”æ¡ˆ];
    D -- ä¸ç›¸å…³ --> C;

    F --> E{ç¬¬ä¸‰å…³: ç­”æ¡ˆä¸ä¸Šä¸‹æ–‡ä¸€è‡´æ€§};
    E -- ä¸€è‡´ --> G;
    E -- ä¸ä¸€è‡´ --> F;

    G --> H{ç¬¬å››å…³: ç­”æ¡ˆæœ€ç»ˆæ•ˆç”¨};
    H -- é«˜ --> I[è¿”å›æœ€ç»ˆç­”æ¡ˆ];
    H -- ä¸­ --> G;
    H -- ä½ --> A;
  

    style A fill:#E3F2FD
    style B fill:#FFF9C4
    style C fill:#E8F5E9
    style D fill:#FFF9C4
    style E fill:#FFF9C4
    style F fill:#F3E5F5
    style G fill:#F3E5F5
    style H fill:#FFF9C4
    style I fill:#C8E6C9
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def determine_if_retrieval(self, query: str) -> str:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢å¤–éƒ¨çŸ¥è¯†"""
    prompt = f"""
    åˆ¤æ–­å›ç­”ä»¥ä¸‹é—®é¢˜æ˜¯å¦éœ€è¦æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼š

    é—®é¢˜ï¼š{query}

    å¦‚æœé—®é¢˜æ˜¯ï¼š
    - äº‹å®æ€§é—®é¢˜ã€éœ€è¦å…·ä½“æ•°æ® â†’ è¿”å› RETRIEVE
    - å¸¸è¯†æ€§é—®é¢˜ã€å¯ç›´æ¥å›ç­” â†’ è¿”å› NO_RETRIEVE

    åªè¿”å› RETRIEVE æˆ– NO_RETRIEVEã€‚
    """

    response = self.llm_client.generate_text(prompt)
    return "RETRIEVE" if "RETRIEVE" in response else "NO_RETRIEVE"

def evaluate_relevance(self, query: str, documents: List[str]) -> List[str]:
    """è¯„ä¼°æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§"""
    relevance_labels = []

    for doc in documents:
        prompt = f"""
        è¯„ä¼°æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼š

        æŸ¥è¯¢ï¼š{query}
        æ–‡æ¡£ï¼š{doc[:300]}...

        è¿”å›ä»¥ä¸‹ä¹‹ä¸€ï¼š
        - RELEVANTï¼šæ–‡æ¡£ç›´æ¥ç›¸å…³ï¼ŒåŒ…å«å›ç­”æ‰€éœ€ä¿¡æ¯
        - PARTIALLY_RELEVANTï¼šæ–‡æ¡£éƒ¨åˆ†ç›¸å…³
        - IRRELEVANTï¼šæ–‡æ¡£ä¸ç›¸å…³
        """

        response = self.llm_client.generate_text(prompt)
        if "RELEVANT" in response and "PARTIALLY" not in response:
            label = "RELEVANT"
        elif "PARTIALLY" in response:
            label = "PARTIALLY_RELEVANT"
        else:
            label = "IRRELEVANT"

        relevance_labels.append(label)

    return relevance_labels

def assess_support(self, answer: str, documents: List[str]) -> str:
    """è¯„ä¼°ç­”æ¡ˆæ˜¯å¦æœ‰æ–‡æ¡£æ”¯æŒ"""
    docs_text = "\n\n".join(documents)

    prompt = f"""
    æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æœ‰æ–‡æ¡£æ”¯æŒï¼š

    ç­”æ¡ˆï¼š{answer}

    æ–‡æ¡£ï¼š{docs_text[:1000]}...

    è¿”å›ä»¥ä¸‹ä¹‹ä¸€ï¼š
    - FULLY_SUPPORTEDï¼šç­”æ¡ˆä¸­çš„æ‰€æœ‰é™ˆè¿°éƒ½æœ‰æ–‡æ¡£æ”¯æŒ
    - PARTIALLY_SUPPORTEDï¼šéƒ¨åˆ†é™ˆè¿°æœ‰æ”¯æŒ
    - NOT_SUPPORTEDï¼šç­”æ¡ˆæ²¡æœ‰æ–‡æ¡£æ”¯æŒæˆ–ä¸æ–‡æ¡£çŸ›ç›¾
    """

    response = self.llm_client.generate_text(prompt)
    if "FULLY_SUPPORTED" in response:
        return "FULLY_SUPPORTED"
    elif "PARTIALLY" in response:
        return "PARTIALLY_SUPPORTED"
    else:
        return "NOT_SUPPORTED"

def rate_utility(self, query: str, answer: str) -> int:
    """è¯„ä¼°ç­”æ¡ˆçš„å®ç”¨æ€§ï¼ˆ1-5åˆ†ï¼‰"""
    prompt = f"""
    è¯„ä¼°ç­”æ¡ˆå¯¹ç”¨æˆ·çš„å®ç”¨æ€§ï¼š

    é—®é¢˜ï¼š{query}
    ç­”æ¡ˆï¼š{answer}

    è¯„åˆ†æ ‡å‡†ï¼ˆ1-5åˆ†ï¼‰ï¼š
    5åˆ†ï¼šå®Œæ•´ã€å‡†ç¡®ã€ç›´æ¥å›ç­”é—®é¢˜ï¼Œéå¸¸æœ‰ç”¨
    4åˆ†ï¼šå›ç­”å‡†ç¡®ï¼Œä¿¡æ¯å……åˆ†
    3åˆ†ï¼šåŸºæœ¬å›ç­”é—®é¢˜ï¼Œä½†ä¸å¤Ÿè¯¦ç»†
    2åˆ†ï¼šéƒ¨åˆ†å›ç­”æˆ–ä¿¡æ¯ä¸è¶³
    1åˆ†ï¼šæœªèƒ½å›ç­”é—®é¢˜æˆ–ä¿¡æ¯é”™è¯¯

    åªè¿”å›1-5çš„æ•°å­—ã€‚
    """

    try:
        response = self.llm_client.generate_text(prompt)
        score = int(response.strip())
        return max(1, min(5, score))
    except:
        return 3  # é»˜è®¤ä¸­ç­‰æ•ˆç”¨

def self_rag(
    self,
    query: str,
    max_iterations: int = 2
) -> Dict[str, Any]:
    """å®Œæ•´çš„Self-RAGæµç¨‹"""
    iteration = 0

    while iteration < max_iterations:
        # æ­¥éª¤1ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
        retrieval_decision = self.determine_if_retrieval(query)

        if retrieval_decision == "RETRIEVE":
            # æ‰§è¡Œæ£€ç´¢
            search_results = self.milvus_client.search_by_text(
                collection_name=self.collection_name,
                text=query,
                limit=5,
                output_fields=["text"],
                metric_type="COSINE",
                embedding_client=self.embedding_client
            )

            documents = [r['entity']['text'] for r in search_results]

            # æ­¥éª¤2ï¼šè¯„ä¼°ç›¸å…³æ€§
            relevance_labels = self.evaluate_relevance(query, documents)

            # è¿‡æ»¤å‡ºç›¸å…³æ–‡æ¡£
            relevant_docs = [
                doc for doc, label in zip(documents, relevance_labels)
                if label in ["RELEVANT", "PARTIALLY_RELEVANT"]
            ]

            if not relevant_docs:
                # æ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨æ¨¡å‹çŸ¥è¯†
                context = ""
            else:
                context = "\n\n".join(relevant_docs)

        else:
            # ä¸éœ€è¦æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ
            documents = []
            context = ""

        # ç”Ÿæˆç­”æ¡ˆ
        if context:
            prompt = f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {query}"
        else:
            prompt = query

        answer = self.llm_client.generate_text(
            prompt,
            system_instruction=self.system_prompt
        )

        # æ­¥éª¤3ï¼šè¯„ä¼°æ”¯æŒåº¦ï¼ˆä»…å½“ä½¿ç”¨äº†æ–‡æ¡£æ—¶ï¼‰
        if documents:
            support_label = self.assess_support(answer, relevant_docs if context else [])

            if support_label == "NOT_SUPPORTED":
                iteration += 1
                continue  # é‡æ–°ç”Ÿæˆ
            elif support_label == "PARTIALLY_SUPPORTED":
                # æ·»åŠ ä¸ç¡®å®šæ€§è¯´æ˜
                answer = f"{answer}\n\næ³¨ï¼šéƒ¨åˆ†ä¿¡æ¯å¯èƒ½éœ€è¦è¿›ä¸€æ­¥éªŒè¯ã€‚"
        else:
            support_label = "NO_RETRIEVAL"

        # æ­¥éª¤4ï¼šè¯„ä¼°æ•ˆç”¨
        utility_score = self.rate_utility(query, answer)

        if utility_score >= 3:
            # æ•ˆç”¨è¶³å¤Ÿï¼Œè¿”å›ç­”æ¡ˆ
            return {
                "query": query,
                "answer": answer,
                "retrieval_used": retrieval_decision == "RETRIEVE",
                "documents_count": len(documents),
                "relevant_docs_count": len(relevant_docs) if documents else 0,
                "support_level": support_label,
                "utility_score": utility_score,
                "iterations": iteration + 1
            }

        # æ•ˆç”¨ä¸è¶³ï¼Œé‡è¯•
        iteration += 1

    # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›å½“å‰æœ€ä½³ç­”æ¡ˆ
    return {
        "query": query,
        "answer": answer,
        "retrieval_used": retrieval_decision == "RETRIEVE",
        "support_level": support_label if documents else "NO_RETRIEVAL",
        "utility_score": utility_score,
        "iterations": max_iterations,
        "warning": "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"
    }
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **å››é‡è¯„ä¼°æœºåˆ¶**ï¼š

   - **Retrieval Necessity**ï¼šé¿å…ä¸å¿…è¦çš„æ£€ç´¢
   - **Relevance**ï¼šç¡®ä¿æ£€ç´¢è´¨é‡
   - **Support**ï¼šéªŒè¯ç­”æ¡ˆçœŸå®æ€§
   - **Utility**ï¼šè¯„ä¼°ç”¨æˆ·æ»¡æ„åº¦
2. **è¿­ä»£ä¼˜åŒ–**ï¼š

   ```python
   while utility_score < threshold and iterations < max:
       # é‡æ–°ç”Ÿæˆæˆ–è°ƒæ•´ç­–ç•¥
       iterations += 1
   ```

   - æœ€å¤šè¿­ä»£Næ¬¡å°è¯•ä¼˜åŒ–ç­”æ¡ˆ
   - é¿å…æ— é™å¾ªç¯è®¾ç½®max_iterations
3. **åˆ†çº§åˆ¤æ–­**ï¼š

   - RELEVANT/PARTIALLY_RELEVANT/IRRELEVANT
   - FULLY_SUPPORTED/PARTIALLY_SUPPORTED/NOT_SUPPORTED
   - 1-5åˆ†æ•ˆç”¨è¯„åˆ†
   - æä¾›ç»†ç²’åº¦çš„è´¨é‡æ§åˆ¶

#### âœ… ä¼˜ç‚¹

- **è´¨é‡ä¿è¯**ï¼šå¤šå±‚è¯„ä¼°ç¡®ä¿é«˜è´¨é‡è¾“å‡º
- **è‡ªæˆ‘çº æ­£**ï¼šå‘ç°é—®é¢˜è‡ªåŠ¨é‡è¯•
- **é€æ˜åº¦é«˜**ï¼šæä¾›è¯¦ç»†çš„è¯„ä¼°ä¿¡æ¯
- **å‡å°‘å¹»è§‰**ï¼šä¸¥æ ¼éªŒè¯ç­”æ¡ˆæ”¯æŒåº¦
- **è‡ªé€‚åº”**ï¼šæ ¹æ®é—®é¢˜ç‰¹ç‚¹è°ƒæ•´ç­–ç•¥

#### âŒ ç¼ºç‚¹

- **æé«˜æˆæœ¬**ï¼šå¤šæ¬¡LLMè¯„ä¼°è°ƒç”¨
- **å»¶è¿Ÿæ˜¾è‘—**ï¼šå¤šè½®è¯„ä¼°å’Œå¯èƒ½çš„è¿­ä»£
- **å¤æ‚åº¦é«˜**ï¼šå®ç°å’Œè°ƒè¯•å›°éš¾
- **è¯„ä¼°å¯èƒ½ä¸å‡†**ï¼šLLMè‡ªè¯„ä¸ä¸€å®šå¯é 
- **èµ„æºå¯†é›†**ï¼šä¸é€‚åˆé«˜å¹¶å‘åœºæ™¯

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- é«˜å‡†ç¡®åº¦è¦æ±‚ï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰
- å¯¹é”™è¯¯é›¶å®¹å¿çš„åœºæ™¯
- éœ€è¦å¯è§£é‡Šæ€§çš„åº”ç”¨
- å†…å®¹å®¡æ ¸å’Œè´¨é‡æ§åˆ¶
- ä¸“ä¸šé¢†åŸŸçŸ¥è¯†é—®ç­”
- å¯¹æˆæœ¬å’Œå»¶è¿Ÿä¸æ•æ„Ÿçš„åœºæ™¯

---

### 6. RerankRAG - é‡æ’åºRAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

RerankRAGé‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢ç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œç²—æ£€ç´¢è·å–è¾ƒå¤šå€™é€‰æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨ä¸“é—¨çš„Rerankæ¨¡å‹å¯¹å€™é€‰ç»“æœè¿›è¡Œç²¾ç»†æ’åºï¼Œæœ€ç»ˆé€‰æ‹©Top-Kæœ€ç›¸å…³çš„æ–‡æ¡£ã€‚è¿™ç§æ–¹æ³•å¹³è¡¡äº†æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **ä¸¤é˜¶æ®µæ£€ç´¢**ï¼šç²—æ£€ç´¢ï¼ˆå‘é‡ï¼‰ + ç²¾æ’åºï¼ˆRerankæ¨¡å‹ï¼‰
- **å€™é€‰æ‰©å±•**ï¼šåˆæ£€ç´¢è·å–æ›´å¤šå€™é€‰ï¼ˆlimit * 2ï¼‰
- **ç²¾ç»†æ’åº**ï¼šä½¿ç”¨Cross-Encoderæˆ–ä¸“é—¨rerankæ¨¡å‹
- **è´¨é‡æå‡**ï¼šRerankæ¨¡å‹è€ƒè™‘query-documentäº¤äº’

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    Start([ç”¨æˆ·æŸ¥è¯¢]) --> Stage1

    subgraph "é˜¶æ®µä¸€: å¿«é€Ÿæµ·é€‰ (é‡å¬å›ç‡)"
        Stage1[å‘é‡æ£€ç´¢<br>è·å–å¤§é‡å€™é€‰ Top-2K]
    end

    Stage1 --> Stage2

    subgraph "é˜¶æ®µäºŒ: ç²¾å‡†æ’åº (é‡ç²¾ç¡®ç‡)"
        Stage2[Rerankæ¨¡å‹ç²¾å‡†æ‰“åˆ†] --> Stage3[æ’åºå¹¶ç­›é€‰ Top-K]
    end

    Stage3 --> BuildContext[æ„å»ºæœ€ç»ˆä¸Šä¸‹æ–‡]
    BuildContext --> LLM[LLMç”Ÿæˆç­”æ¡ˆ]
    LLM --> Response([è¿”å›ç­”æ¡ˆ])

    %% -- æ ·å¼å®šä¹‰ --
    style Start fill:#E3F2FD
    style Stage1 fill:#F3E5F5
    style Stage2 fill:#F3E5F5
    style Stage3 fill:#FFF9C4
    style BuildContext fill:#E8F5E9
    style LLM fill:#F3E5F5
    style Response fill:#C8E6C9
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def search_and_rerank(
    self,
    query: str,
    limit: int = 5,
    rerank_multiplier: int = 2
) -> List[Dict[str, Any]]:
    """ä¸¤é˜¶æ®µæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + Rerank"""

    # é˜¶æ®µ1ï¼šç²—æ£€ç´¢ - è·å–æ›´å¤šå€™é€‰
    initial_limit = limit * rerank_multiplier  # ä¾‹å¦‚ï¼šéœ€è¦5ä¸ªï¼Œå…ˆå–10ä¸ª

    search_results = self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=query,
        limit=initial_limit,  # æ‰©å¤§å€™é€‰æ± 
        output_fields=["text", "source", "chunk_index"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

    if not search_results:
        return []

    # æå–æ–‡æ¡£æ–‡æœ¬
    documents = [
        result['entity']['text']
        for result in search_results
    ]

    # é˜¶æ®µ2ï¼šç²¾æ’åº
    reranked_results = self._rerank_documents(query, search_results, documents)

    # è¿”å›Top-K
    return reranked_results[:limit]

def _rerank_documents(
    self,
    query: str,
    search_results: List[Dict],
    documents: List[str]
) -> List[Dict[str, Any]]:
    """ä½¿ç”¨Rerankæ¨¡å‹é‡æ–°æ’åº"""

    # æ–¹æ³•1ï¼šä½¿ç”¨ä¸“é—¨çš„Rerankæ¨¡å‹ï¼ˆå¦‚Cohere Rerank, jina-rerankerï¼‰
    # è¿™é‡Œä½¿ç”¨LLMæ¨¡æ‹Ÿrerankï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ä¸“é—¨æ¨¡å‹ï¼‰

    rerank_scores = []
    for doc in documents:
        # ä½¿ç”¨LLMè¯„ä¼°æŸ¥è¯¢-æ–‡æ¡£ç›¸å…³æ€§
        score = self._compute_rerank_score(query, doc)
        rerank_scores.append(score)

    # å°†rerankåˆ†æ•°æ·»åŠ åˆ°ç»“æœä¸­
    for result, score in zip(search_results, rerank_scores):
        result['rerank_score'] = score
        # ä¿ç•™åŸå§‹å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
        result['vector_score'] = result.get('score', 1 - result.get('distance', 0))

    # æŒ‰rerankåˆ†æ•°æ’åº
    reranked = sorted(
        search_results,
        key=lambda x: x['rerank_score'],
        reverse=True
    )

    return reranked

def _compute_rerank_score(self, query: str, document: str) -> float:
    """è®¡ç®—rerankåˆ†æ•°ï¼ˆå®é™…åº”ä½¿ç”¨Cross-Encoderæ¨¡å‹ï¼‰"""
    # å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ï¼š
    # - sentence-transformersçš„Cross-Encoder
    # - Cohere Rerank API
    # - jina-reranker
    # - ms-marco-MiniLM-L-12-v2

    # è¿™é‡Œç”¨LLMæ¨¡æ‹Ÿï¼ˆä»…ä¸ºç¤ºä¾‹ï¼‰
    prompt = f"""
    è¯„ä¼°æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼ˆ0-100åˆ†ï¼‰ï¼š

    æŸ¥è¯¢ï¼š{query}
    æ–‡æ¡£ï¼š{document[:500]}

    è€ƒè™‘ï¼š
    - è¯­ä¹‰åŒ¹é…ç¨‹åº¦
    - ä¿¡æ¯å®Œæ•´æ€§
    - å›ç­”é—®é¢˜çš„ç›´æ¥ç¨‹åº¦

    åªè¿”å›0-100çš„æ•´æ•°åˆ†æ•°ã€‚
    """

    try:
        response = self.llm_client.generate_text(prompt)
        score = float(response.strip())
        return max(0.0, min(100.0, score)) / 100.0
    except:
        return 0.5

# å®é™…ç”Ÿäº§ç¯å¢ƒçš„Rerankå®ç°ç¤ºä¾‹ï¼š
def _rerank_with_cross_encoder(
    self,
    query: str,
    documents: List[str]
) -> List[float]:
    """ä½¿ç”¨Cross-Encoderæ¨¡å‹è¿›è¡Œrerankï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰"""
    # ç¤ºä¾‹ï¼šä½¿ç”¨sentence-transformersçš„Cross-Encoder
    """
    from sentence_transformers import CrossEncoder

    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

    # å‡†å¤‡query-documentå¯¹
    pairs = [[query, doc] for doc in documents]

    # æ‰¹é‡è®¡ç®—åˆ†æ•°
    scores = model.predict(pairs)

    return scores.tolist()
    """
    pass

def query(self, question: str, limit: int = 3) -> str:
    """å®Œæ•´çš„Rerank RAGæŸ¥è¯¢æµç¨‹"""
    # 1. ä¸¤é˜¶æ®µæ£€ç´¢
    reranked_results = self.search_and_rerank(question, limit)

    if not reranked_results:
        return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

    # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨rerankåçš„ç»“æœï¼‰
    context_parts = []
    for i, result in enumerate(reranked_results):
        text = result['entity']['text']
        rerank_score = result['rerank_score']
        vector_score = result['vector_score']

        context_parts.append(
            f"æ–‡æ¡£{i+1} [å‘é‡åˆ†æ•°:{vector_score:.3f}, Rerankåˆ†æ•°:{rerank_score:.3f}]:\n{text}"
        )

    context = "\n\n".join(context_parts)

    # 3. ç”Ÿæˆç­”æ¡ˆ
    return self.llm_client.generate_text(
        f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}",
        system_instruction=self.system_prompt
    )
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **ä¸¤é˜¶æ®µæ£€ç´¢ç†è®º**ï¼š

   - **ç¬¬ä¸€é˜¶æ®µï¼ˆRecallï¼‰**ï¼šå‘é‡æ£€ç´¢å¿«é€Ÿä½†ä¸å¤Ÿç²¾ç¡®ï¼Œç”¨äºå¬å›
   - **ç¬¬äºŒé˜¶æ®µï¼ˆPrecisionï¼‰**ï¼šRerankæ¨¡å‹æ…¢ä½†ç²¾ç¡®ï¼Œç”¨äºç²¾æ’
   - ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼šæ•ˆç‡ + å‡†ç¡®æ€§
2. **Cross-Encoder vs Bi-Encoder**ï¼š

   - **Bi-Encoder**ï¼ˆå‘é‡æ£€ç´¢ï¼‰ï¼š

     ```
     encode(query) âŠ— encode(doc)
     ç‹¬ç«‹ç¼–ç ï¼Œå¿«é€Ÿä½†ä¿¡æ¯æŸå¤±
     ```
   - **Cross-Encoder**ï¼ˆRerankï¼‰ï¼š

     ```
     encode(query + doc together)
     è”åˆç¼–ç ï¼Œæ…¢ä½†æ›´å‡†ç¡®
     ```
3. **å€™é€‰æ‰©å±•ç­–ç•¥**ï¼š

   ```python
   initial_limit = target_limit * multiplier
   # multiplieré€šå¸¸ä¸º2-3
   # å¹³è¡¡å¬å›ç‡å’Œè®¡ç®—æˆæœ¬
   ```
4. **åˆ†æ•°èåˆï¼ˆå¯é€‰ï¼‰**ï¼š

   ```python
   final_score = Î± * vector_score + Î² * rerank_score
   # Î± + Î² = 1
   # æˆ–è€…åªç”¨rerank_score
   ```

#### âœ… ä¼˜ç‚¹

- **æå‡å‡†ç¡®æ€§**ï¼šRerankæ˜¾è‘—æé«˜Top-Kè´¨é‡
- **ä¿æŒæ•ˆç‡**ï¼šå‘é‡æ£€ç´¢å¿«é€Ÿç­›é€‰å€™é€‰
- **çµæ´»æ€§å¼º**ï¼šå¯ä»¥è½»æ¾æ›´æ¢rerankæ¨¡å‹
- **æ•ˆæœæ˜¾è‘—**ï¼šé€šå¸¸æå‡10-20%çš„å‡†ç¡®ç‡
- **å³æ’å³ç”¨**ï¼šå®¹æ˜“é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

#### âŒ ç¼ºç‚¹

- **é¢å¤–è®¡ç®—**ï¼šRerankå¢åŠ è®¡ç®—å¼€é”€
- **å»¶è¿Ÿå¢åŠ **ï¼šä¸¤é˜¶æ®µå¤„ç†å¢åŠ å“åº”æ—¶é—´
- **æ¨¡å‹ä¾èµ–**ï¼šéœ€è¦é¢å¤–çš„Rerankæ¨¡å‹
- **å†…å­˜å ç”¨**ï¼šéœ€è¦åŠ è½½é¢å¤–æ¨¡å‹
- **æˆæœ¬ä¸Šå‡**ï¼šAPIè°ƒç”¨æˆ–GPUèµ„æº

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- å¯¹æ£€ç´¢å‡†ç¡®åº¦è¦æ±‚é«˜çš„åœºæ™¯
- å¯ä»¥å®¹å¿è½»å¾®å»¶è¿Ÿå¢åŠ 
- æœ‰è®¡ç®—èµ„æºæ”¯æŒRerankæ¨¡å‹
- éœ€è¦æå‡Top-Kç»“æœè´¨é‡
- é€šç”¨RAGç³»ç»Ÿçš„å‡çº§æ–¹æ¡ˆ
- æœç´¢å¼•æ“å’Œæ¨èç³»ç»Ÿ

---

### 7. FusionRAG - æ··åˆæ£€ç´¢RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

FusionRAGç»“åˆäº†ç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰å’Œå¯†é›†æ£€ç´¢ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰ä¸¤ç§æ–¹æ³•ï¼Œä½¿ç”¨å€’æ•°æ’åèåˆï¼ˆReciprocal Rank Fusion, RRFï¼‰ç®—æ³•åˆå¹¶ä¸¤ç§æ£€ç´¢ç»“æœã€‚BM25æ“…é•¿ç²¾ç¡®å…³é”®è¯åŒ¹é…ï¼Œå‘é‡æ£€ç´¢æ“…é•¿è¯­ä¹‰ç†è§£ï¼Œä¸¤è€…äº’è¡¥å¯ä»¥æ˜¾è‘—æå‡æ£€ç´¢è´¨é‡ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **åŒè·¯æ£€ç´¢**ï¼šBM25ç¨€ç–æ£€ç´¢ + å‘é‡å¯†é›†æ£€ç´¢
- **ä¼˜åŠ¿äº’è¡¥**ï¼šå…³é”®è¯ç²¾ç¡®åŒ¹é… + è¯­ä¹‰ç†è§£
- **RRFèåˆ**ï¼šåŸºäºæ’åçš„å…¬å¹³èåˆç®—æ³•
- **é²æ£’æ€§å¼º**ï¼šå•ä¸€æ–¹æ³•å¤±æ•ˆæ—¶å¦ä¸€ä¸ªå¯è¡¥å¿

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    ç”¨æˆ·æŸ¥è¯¢ --> æŸ¥è¯¢å¤„ç†
    æŸ¥è¯¢å¤„ç† --> ç¨€ç–æ£€ç´¢
    æŸ¥è¯¢å¤„ç† --> å¯†é›†æ£€ç´¢
    ç¨€ç–æ£€ç´¢ --> BM25æ£€ç´¢
    å¯†é›†æ£€ç´¢ --> å‘é‡æ£€ç´¢
    BM25æ£€ç´¢ --> ç»“æœèåˆ
    å‘é‡æ£€ç´¢ --> ç»“æœèåˆ
    ç»“æœèåˆ --> æ’åºå¹¶é€‰æ‹©æœ€ä¼˜ç»“æœ
    æ’åºå¹¶é€‰æ‹©æœ€ä¼˜ç»“æœ --> æ„å»ºä¸Šä¸‹æ–‡
    æ„å»ºä¸Šä¸‹æ–‡ --> LLMç”Ÿæˆç­”æ¡ˆ
    LLMç”Ÿæˆç­”æ¡ˆ --> è¿”å›ç­”æ¡ˆ
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
from rank_bm25 import BM25Okapi
import numpy as np

def create_bm25_index(self, documents: List[str]) -> BM25Okapi:
    """åˆ›å»ºBM25ç´¢å¼•"""
    # åˆ†è¯ï¼ˆä¸­æ–‡éœ€è¦jiebaç­‰åˆ†è¯å·¥å…·ï¼‰
    tokenized_corpus = [doc.split() for doc in documents]

    # åˆ›å»ºBM25ç´¢å¼•
    bm25 = BM25Okapi(tokenized_corpus)

    return bm25

def bm25_search(
    self,
    query: str,
    bm25_index: BM25Okapi,
    documents: List[str],
    limit: int = 10
) -> List[Dict[str, Any]]:
    """æ‰§è¡ŒBM25æ£€ç´¢"""
    # æŸ¥è¯¢åˆ†è¯
    tokenized_query = query.split()

    # è®¡ç®—BM25åˆ†æ•°
    scores = bm25_index.get_scores(tokenized_query)

    # è·å–Top-Kç´¢å¼•
    top_indices = np.argsort(scores)[::-1][:limit]

    # æ„å»ºç»“æœ
    results = []
    for rank, idx in enumerate(top_indices):
        results.append({
            'doc_id': idx,
            'text': documents[idx],
            'bm25_score': float(scores[idx]),
            'bm25_rank': rank + 1
        })

    return results

def vector_search(
    self,
    query: str,
    limit: int = 10
) -> List[Dict[str, Any]]:
    """æ‰§è¡Œå‘é‡æ£€ç´¢"""
    results = self.milvus_client.search_by_text(
        collection_name=self.collection_name,
        text=query,
        limit=limit,
        output_fields=["text", "source", "chunk_index"],
        metric_type="COSINE",
        embedding_client=self.embedding_client
    )

    # æ·»åŠ æ’åä¿¡æ¯
    for rank, result in enumerate(results):
        result['vector_rank'] = rank + 1
        result['vector_score'] = result.get('score', 1 - result.get('distance', 0))

    return results

def reciprocal_rank_fusion(
    self,
    bm25_results: List[Dict],
    vector_results: List[Dict],
    k: int = 60  # RRFå¸¸æ•°ï¼Œé€šå¸¸å–60
) -> List[Dict[str, Any]]:
    """å€’æ•°æ’åèåˆç®—æ³•"""
    # åˆ›å»ºæ–‡æ¡£IDåˆ°èåˆåˆ†æ•°çš„æ˜ å°„
    fusion_scores = {}

    # å¤„ç†BM25ç»“æœ
    for result in bm25_results:
        doc_id = result.get('doc_id') or result.get('id')
        rank = result['bm25_rank']

        # RRFå…¬å¼ï¼šscore = 1 / (k + rank)
        rrf_score = 1.0 / (k + rank)

        if doc_id not in fusion_scores:
            fusion_scores[doc_id] = {
                'doc_id': doc_id,
                'text': result['text'],
                'rrf_score': 0.0,
                'bm25_rank': rank,
                'vector_rank': None,
                'bm25_score': result.get('bm25_score'),
                'vector_score': None
            }

        fusion_scores[doc_id]['rrf_score'] += rrf_score

    # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
    for result in vector_results:
        doc_id = result.get('id') or result['entity'].get('id')
        rank = result['vector_rank']

        rrf_score = 1.0 / (k + rank)

        if doc_id not in fusion_scores:
            fusion_scores[doc_id] = {
                'doc_id': doc_id,
                'text': result['entity']['text'],
                'rrf_score': 0.0,
                'bm25_rank': None,
                'vector_rank': rank,
                'bm25_score': None,
                'vector_score': result['vector_score']
            }
        else:
            fusion_scores[doc_id]['vector_rank'] = rank
            fusion_scores[doc_id]['vector_score'] = result['vector_score']

        fusion_scores[doc_id]['rrf_score'] += rrf_score

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
    fused_results = list(fusion_scores.values())
    fused_results.sort(key=lambda x: x['rrf_score'], reverse=True)

    return fused_results

def fusion_search(
    self,
    query: str,
    limit: int = 5,
    bm25_limit: int = 10,
    vector_limit: int = 10
) -> List[Dict[str, Any]]:
    """æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡ + RRFèåˆ"""

    # 1. è·å–æ‰€æœ‰æ–‡æ¡£ï¼ˆç”¨äºBM25ï¼‰
    all_docs = self._get_all_documents()

    # 2. åˆ›å»ºBM25ç´¢å¼•ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥é¢„å…ˆæ„å»ºå¹¶ç¼“å­˜ï¼‰
    bm25_index = self.create_bm25_index(all_docs)

    # 3. BM25æ£€ç´¢
    bm25_results = self.bm25_search(query, bm25_index, all_docs, bm25_limit)

    # 4. å‘é‡æ£€ç´¢
    vector_results = self.vector_search(query, vector_limit)

    # 5. RRFèåˆ
    fused_results = self.reciprocal_rank_fusion(
        bm25_results,
        vector_results,
        k=60
    )

    # 6. è¿”å›Top-K
    return fused_results[:limit]

def query(self, question: str, limit: int = 3) -> str:
    """å®Œæ•´çš„Fusion RAGæŸ¥è¯¢æµç¨‹"""
    # 1. æ··åˆæ£€ç´¢
    fusion_results = self.fusion_search(question, limit)

    if not fusion_results:
        return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, result in enumerate(fusion_results):
        text = result['text']
        rrf_score = result['rrf_score']
        bm25_rank = result['bm25_rank'] or '-'
        vector_rank = result['vector_rank'] or '-'

        context_parts.append(
            f"æ–‡æ¡£{i+1} [RRF:{rrf_score:.4f}, BM25æ’å:{bm25_rank}, å‘é‡æ’å:{vector_rank}]:\n{text}"
        )

    context = "\n\n".join(context_parts)

    # 3. ç”Ÿæˆç­”æ¡ˆ
    return self.llm_client.generate_text(
        f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}",
        system_instruction=self.system_prompt
    )
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **BM25ç®—æ³•**ï¼š

   ```
   BM25(q,d) = Î£ IDF(qi) Ã— [f(qi,d) Ã— (k1 + 1)] / [f(qi,d) + k1 Ã— (1 - b + b Ã— |d|/avgdl)]

   å…¶ä¸­ï¼š
   - f(qi,d): è¯qiåœ¨æ–‡æ¡£dä¸­çš„é¢‘ç‡
   - |d|: æ–‡æ¡£dçš„é•¿åº¦
   - avgdl: å¹³å‡æ–‡æ¡£é•¿åº¦
   - k1, b: è°ƒèŠ‚å‚æ•°ï¼ˆé€šå¸¸k1=1.5, b=0.75ï¼‰
   - IDF(qi): é€†æ–‡æ¡£é¢‘ç‡
   ```
2. **å€’æ•°æ’åèåˆï¼ˆRRFï¼‰**ï¼š

   ```python
   RRF_score(d) = Î£ [1 / (k + rank_i(d))]

   å…¶ä¸­ï¼š
   - rank_i(d): æ–‡æ¡£dåœ¨ç¬¬iä¸ªæ’ååˆ—è¡¨ä¸­çš„ä½ç½®
   - k: å¸¸æ•°ï¼ˆé€šå¸¸ä¸º60ï¼‰
   - å¯¹æ‰€æœ‰æ’ååˆ—è¡¨æ±‚å’Œ
   ```

   RRFä¼˜ç‚¹ï¼š

   - ä¸éœ€è¦å½’ä¸€åŒ–åˆ†æ•°
   - å¯¹ä¸åŒæ£€ç´¢ç³»ç»Ÿçš„åˆ†æ•°èŒƒå›´ä¸æ•æ„Ÿ
   - ç®€å•æœ‰æ•ˆ
3. **èåˆç­–ç•¥å¯¹æ¯”**ï¼š

   - **Linear Combination**: `Î±*score1 + Î²*score2` ï¼ˆéœ€è¦å½’ä¸€åŒ–ï¼‰
   - **RRF**: `Î£ 1/(k+rank)` ï¼ˆåŸºäºæ’åï¼Œæ›´é²æ£’ï¼‰
   - **CombSUM/CombMNZ**: å…¶ä»–èåˆæ–¹æ³•
4. **ä¸ºä»€ä¹ˆk=60**ï¼š

   - ç»éªŒå€¼ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½
   - å¹³è¡¡é«˜æ’åå’Œä½æ’åæ–‡æ¡£çš„å½±å“
   - å¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨è°ƒæ•´

#### âœ… ä¼˜ç‚¹

- **å¬å›ç‡æå‡**ï¼šä¸¤ç§æ–¹æ³•äº’è¡¥ï¼Œè¦†ç›–æ›´å…¨é¢
- **é²æ£’æ€§å¼º**ï¼šå•ä¸€æ–¹æ³•å¤±æ•ˆæ—¶å¦ä¸€ä¸ªå¯è¡¥å¿
- **å…³é”®è¯å‹å¥½**ï¼šBM25æ“…é•¿ç²¾ç¡®åŒ¹é…
- **è¯­ä¹‰ç†è§£**ï¼šå‘é‡æ£€ç´¢æ•è·è¯­ä¹‰ç›¸ä¼¼
- **æ— éœ€è°ƒå‚**ï¼šRRFç®—æ³•ç®€å•ï¼Œk=60é€‚ç”¨å¤§å¤šæ•°åœºæ™¯

#### âŒ ç¼ºç‚¹

- **å¤æ‚åº¦é«˜**ï¼šéœ€è¦ç»´æŠ¤ä¸¤å¥—ç´¢å¼•
- **è®¡ç®—å¼€é”€å¤§**ï¼šä¸¤æ¬¡æ£€ç´¢å’Œèåˆ
- **å»¶è¿Ÿå¢åŠ **ï¼šä¸²è¡Œæ‰§è¡Œä¸¤ç§æ£€ç´¢
- **å­˜å‚¨æˆæœ¬**ï¼šBM25ç´¢å¼•é¢å¤–å ç”¨ç©ºé—´
- **ä¸­æ–‡æ”¯æŒ**ï¼šBM25éœ€è¦å¥½çš„åˆ†è¯å™¨

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- éœ€è¦åŒæ—¶æ”¯æŒå…³é”®è¯å’Œè¯­ä¹‰æœç´¢
- ç”¨æˆ·æŸ¥è¯¢å¤šæ ·åŒ–ï¼ˆç²¾ç¡®+æ¨¡ç³Šï¼‰
- å¯¹å¬å›ç‡è¦æ±‚é«˜
- ä¸“ä¸šæœ¯è¯­å’Œå£è¯­æ··åˆåœºæ™¯
- å¤šè¯­è¨€æˆ–è·¨é¢†åŸŸåº”ç”¨
- é€šç”¨æœç´¢å¼•æ“

---

### 8. QueryTransformRAG - æŸ¥è¯¢è½¬æ¢RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

QueryTransformRAGé€šè¿‡è½¬æ¢ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢æ¥æå‡æ£€ç´¢è´¨é‡ã€‚ç³»ç»Ÿæä¾›ä¸‰ç§è½¬æ¢ç­–ç•¥ï¼š1) Query Rewriteï¼ˆé‡å†™ï¼‰å¢å¼ºæŸ¥è¯¢çš„æ˜ç¡®æ€§å’Œå®Œæ•´æ€§ï¼›2) Step-back Promptingï¼ˆåé€€ï¼‰å°†å…·ä½“é—®é¢˜æ³›åŒ–ä¸ºé«˜å±‚æ¦‚å¿µï¼›3) Query Decompositionï¼ˆåˆ†è§£ï¼‰å°†å¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œåˆ†åˆ«æ£€ç´¢åç»¼åˆå›ç­”ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **æŸ¥è¯¢ä¼˜åŒ–**ï¼šæ”¹è¿›åŸå§‹æŸ¥è¯¢çš„è¡¨è¾¾æ–¹å¼
- **ä¸‰ç§ç­–ç•¥**ï¼šRewriteï¼ˆå¢å¼ºï¼‰ã€Step-backï¼ˆæ³›åŒ–ï¼‰ã€Decomposeï¼ˆåˆ†è§£ï¼‰
- **è‡ªé€‚åº”é€‰æ‹©**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹ç‚¹é€‰æ‹©åˆé€‚ç­–ç•¥
- **å¤šè§’åº¦æ£€ç´¢**ï¼šä»ä¸åŒè§’åº¦è·å–ä¿¡æ¯

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TD
    ç”¨æˆ·åŸå§‹æŸ¥è¯¢ --> åˆ†ææŸ¥è¯¢ç±»å‹
    åˆ†ææŸ¥è¯¢ç±»å‹ --> æŸ¥è¯¢é‡å†™
    åˆ†ææŸ¥è¯¢ç±»å‹ --> åé€€æ³›åŒ–
    åˆ†ææŸ¥è¯¢ç±»å‹ --> æŸ¥è¯¢åˆ†è§£

    æŸ¥è¯¢é‡å†™ --> å•æ¬¡æ£€ç´¢
    åé€€æ³›åŒ– --> æ£€ç´¢èƒŒæ™¯å’Œå…·ä½“ä¿¡æ¯
    æŸ¥è¯¢åˆ†è§£ --> å¹¶è¡Œæ£€ç´¢å­é—®é¢˜

    å•æ¬¡æ£€ç´¢ --> æ•´åˆç»“æœ
    æ£€ç´¢èƒŒæ™¯å’Œå…·ä½“ä¿¡æ¯ --> æ•´åˆç»“æœ
    å¹¶è¡Œæ£€ç´¢å­é—®é¢˜ --> æ•´åˆç»“æœ

    æ•´åˆç»“æœ --> æ„å»ºä¸Šä¸‹æ–‡
    æ„å»ºä¸Šä¸‹æ–‡ --> LLMç”Ÿæˆç­”æ¡ˆ
    LLMç”Ÿæˆç­”æ¡ˆ --> è¿”å›ç­”æ¡ˆ
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def rewrite_query(self, query: str) -> str:
    """ç­–ç•¥1ï¼šé‡å†™æŸ¥è¯¢ï¼Œä½¿å…¶æ›´æ˜ç¡®å’Œå®Œæ•´"""
    rewrite_prompt = f"""
    è¯·æ”¹å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´æ¸…æ™°ã€æ˜ç¡®å’Œå®Œæ•´ï¼š

    åŸå§‹æŸ¥è¯¢ï¼š{query}

    æ”¹å†™è¦æ±‚ï¼š
    1. æ·»åŠ å¿…è¦çš„ä¸Šä¸‹æ–‡å’Œå…³é”®è¯
    2. ä¿®æ­£è¯­æ³•å’Œè¡¨è¾¾
    3. ä½¿æ„å›¾æ›´æ˜ç¡®
    4. ä¿æŒç®€æ´ï¼Œä¸è¦è¿‡åº¦æ‰©å±•

    æ”¹å†™åçš„æŸ¥è¯¢ï¼š
    """

    rewritten = self.llm_client.generate_text(rewrite_prompt)
    return rewritten.strip()

def generate_step_back_query(self, query: str) -> str:
    """ç­–ç•¥2ï¼šç”Ÿæˆåé€€å¼ï¼ˆæ›´é«˜å±‚ï¼‰çš„æŸ¥è¯¢"""
    stepback_prompt = f"""
    å¯¹äºä»¥ä¸‹å…·ä½“é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæ›´é«˜å±‚æ¬¡ã€æ›´æŠ½è±¡çš„é—®é¢˜ï¼Œ
    ä»¥å¸®åŠ©è·å–èƒŒæ™¯çŸ¥è¯†å’Œä¸€èˆ¬åŸç†ã€‚

    å…·ä½“é—®é¢˜ï¼š{query}

    ç¤ºä¾‹ï¼š
    - å…·ä½“é—®é¢˜ï¼š"ä¸ºä»€ä¹ˆiPhone 13ç”µæ± ç»­èˆªæ¯”iPhone 12é•¿ï¼Ÿ"
    - é«˜å±‚é—®é¢˜ï¼š"æ™ºèƒ½æ‰‹æœºç”µæ± æŠ€æœ¯çš„å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"

    - å…·ä½“é—®é¢˜ï¼š"å¦‚ä½•åœ¨Pythonä¸­å®ç°å¿«é€Ÿæ’åºï¼Ÿ"
    - é«˜å±‚é—®é¢˜ï¼š"ä»€ä¹ˆæ˜¯æ’åºç®—æ³•åŠå…¶åˆ†ç±»ï¼Ÿ"

    é«˜å±‚é—®é¢˜ï¼š
    """

    stepback_query = self.llm_client.generate_text(stepback_prompt)
    return stepback_query.strip()

def decompose_query(self, query: str) -> List[str]:
    """ç­–ç•¥3ï¼šå°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜"""
    decompose_prompt = f"""
    å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸º3-5ä¸ªæ›´ç®€å•çš„å­é—®é¢˜ã€‚
    æ¯ä¸ªå­é—®é¢˜åº”è¯¥ç‹¬ç«‹å¯ç­”ï¼Œä¸”åˆåœ¨ä¸€èµ·èƒ½å®Œæ•´å›ç­”åŸé—®é¢˜ã€‚

    å¤æ‚é—®é¢˜ï¼š{query}

    è¯·ä»¥ç¼–å·åˆ—è¡¨å½¢å¼è¿”å›å­é—®é¢˜ï¼š
    1. [å­é—®é¢˜1]
    2. [å­é—®é¢˜2]
    ...
    """

    response = self.llm_client.generate_text(decompose_prompt)

    # è§£æå­é—®é¢˜
    sub_queries = []
    for line in response.split('\n'):
        line = line.strip()
        if line and (line[0].isdigit() or line.startswith('-')):
            # ç§»é™¤ç¼–å·
            sub_query = line.split('.', 1)[-1].strip()
            sub_query = sub_query.lstrip('-').strip()
            if sub_query:
                sub_queries.append(sub_query)

    return sub_queries

def search_with_transformation(
    self,
    query: str,
    strategy: str = "auto",  # "rewrite", "stepback", "decompose", "auto"
    limit: int = 5
) -> Dict[str, Any]:
    """ä½¿ç”¨æŸ¥è¯¢è½¬æ¢ç­–ç•¥è¿›è¡Œæ£€ç´¢"""

    # è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
    if strategy == "auto":
        strategy = self._select_strategy(query)

    if strategy == "rewrite":
        # ç­–ç•¥1ï¼šé‡å†™æŸ¥è¯¢
        rewritten_query = self.rewrite_query(query)

        results = self.milvus_client.search_by_text(
            collection_name=self.collection_name,
            text=rewritten_query,
            limit=limit,
            output_fields=["text", "source"],
            metric_type="COSINE",
            embedding_client=self.embedding_client
        )

        return {
            "strategy": "rewrite",
            "original_query": query,
            "transformed_query": rewritten_query,
            "results": results
        }

    elif strategy == "stepback":
        # ç­–ç•¥2ï¼šåé€€å¼æŸ¥è¯¢
        stepback_query = self.generate_step_back_query(query)

        # å…ˆæ£€ç´¢é«˜å±‚çŸ¥è¯†
        background_results = self.milvus_client.search_by_text(
            collection_name=self.collection_name,
            text=stepback_query,
            limit=limit,
            output_fields=["text", "source"],
            metric_type="COSINE",
            embedding_client=self.embedding_client
        )

        # å†æ£€ç´¢å…·ä½“ä¿¡æ¯
        specific_results = self.milvus_client.search_by_text(
            collection_name=self.collection_name,
            text=query,
            limit=limit,
            output_fields=["text", "source"],
            metric_type="COSINE",
            embedding_client=self.embedding_client
        )

        # åˆå¹¶ç»“æœï¼ˆå»é‡ï¼‰
        all_results = self._merge_results(background_results, specific_results)

        return {
            "strategy": "stepback",
            "original_query": query,
            "stepback_query": stepback_query,
            "results": all_results
        }

    elif strategy == "decompose":
        # ç­–ç•¥3ï¼šæŸ¥è¯¢åˆ†è§£
        sub_queries = self.decompose_query(query)

        # ä¸ºæ¯ä¸ªå­é—®é¢˜æ£€ç´¢
        sub_results = []
        for sub_query in sub_queries:
            sub_result = self.milvus_client.search_by_text(
                collection_name=self.collection_name,
                text=sub_query,
                limit=limit,
                output_fields=["text", "source"],
                metric_type="COSINE",
                embedding_client=self.embedding_client
            )
            sub_results.append({
                "sub_query": sub_query,
                "results": sub_result
            })

        # èšåˆæ‰€æœ‰ç»“æœ
        all_results = self._aggregate_sub_results(sub_results)

        return {
            "strategy": "decompose",
            "original_query": query,
            "sub_queries": sub_queries,
            "sub_results": sub_results,
            "aggregated_results": all_results
        }

def _select_strategy(self, query: str) -> str:
    """è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„è½¬æ¢ç­–ç•¥"""
    selection_prompt = f"""
    ä¸ºä»¥ä¸‹æŸ¥è¯¢é€‰æ‹©æœ€åˆé€‚çš„å¤„ç†ç­–ç•¥ï¼š

    æŸ¥è¯¢ï¼š{query}

    ç­–ç•¥é€‰é¡¹ï¼š
    - rewrite: æŸ¥è¯¢è¡¨è¾¾ä¸æ¸…æ™°ï¼Œéœ€è¦é‡å†™
    - stepback: å…·ä½“é—®é¢˜ï¼Œéœ€è¦èƒŒæ™¯çŸ¥è¯†
    - decompose: å¤æ‚é—®é¢˜ï¼ŒåŒ…å«å¤šä¸ªæ–¹é¢

    åªè¿”å›ç­–ç•¥åç§°ï¼ˆrewrite/stepback/decomposeï¼‰ã€‚
    """

    response = self.llm_client.generate_text(selection_prompt).strip().lower()

    if "decompose" in response:
        return "decompose"
    elif "stepback" in response:
        return "stepback"
    else:
        return "rewrite"

def query(self, question: str, strategy: str = "auto", limit: int = 3) -> str:
    """å®Œæ•´çš„Query Transform RAGæµç¨‹"""
    # 1. ä½¿ç”¨è½¬æ¢ç­–ç•¥æ£€ç´¢
    search_result = self.search_with_transformation(question, strategy, limit)

    # 2. æ„å»ºä¸Šä¸‹æ–‡
    if search_result["strategy"] == "decompose":
        # åˆ†è§£ç­–ç•¥ï¼šç»„ç»‡å­é—®é¢˜çš„ç»“æœ
        context_parts = []
        for sub_result in search_result["sub_results"]:
            sub_query = sub_result["sub_query"]
            context_parts.append(f"\n## {sub_query}")
            for i, result in enumerate(sub_result["results"][:2]):
                text = result['entity']['text']
                context_parts.append(f"{text[:200]}...")

        context = "\n\n".join(context_parts)

        # è¦æ±‚LLMç»¼åˆæ‰€æœ‰å­é—®é¢˜çš„ç­”æ¡ˆ
        prompt = f"""
        åŸé—®é¢˜ï¼š{question}

        å­é—®é¢˜åŠå…¶ç›¸å…³ä¿¡æ¯ï¼š
        {context}

        è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œå®Œæ•´å›ç­”åŸé—®é¢˜ã€‚
        """
    else:
        # Rewriteæˆ–Stepbackç­–ç•¥ï¼šæ ‡å‡†ä¸Šä¸‹æ–‡
        results = search_result["results"]
        context = "\n\n".join([
            f"æ–‡æ¡£{i+1}:\n{r['entity']['text']}"
            for i, r in enumerate(results[:limit])
        ])
        prompt = f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}"

    # 3. ç”Ÿæˆç­”æ¡ˆ
    answer = self.llm_client.generate_text(
        prompt,
        system_instruction=self.system_prompt
    )

    return answer
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **Query RewriteåŸç†**ï¼š

   - å¢å¼ºæŸ¥è¯¢çš„ä¿¡æ¯é‡
   - æ·»åŠ åŒä¹‰è¯å’Œç›¸å…³è¯
   - ä¿®æ­£æ‹¼å†™å’Œè¯­æ³•
   - ä½¿æ„å›¾æ›´æ˜ç¡®
2. **Step-back Prompting**ï¼š

   ```
   å…·ä½“é—®é¢˜ â†’ æŠ½è±¡æ¦‚å¿µ â†’ èƒŒæ™¯çŸ¥è¯† â†’ æ›´å¥½ç†è§£å…·ä½“é—®é¢˜

   ä¾‹å¦‚ï¼š
   "Pythonä¸­åˆ—è¡¨æ¨å¯¼å¼å¦‚ä½•å·¥ä½œï¼Ÿ"
   â†’ Step-back: "ä»€ä¹ˆæ˜¯Pythonçš„è¯­æ³•ç³–å’Œåˆ—è¡¨æ“ä½œï¼Ÿ"
   â†’ è·å–æ›´å…¨é¢çš„èƒŒæ™¯çŸ¥è¯†
   ```
3. **Query Decomposition**ï¼š

   ```
   å¤æ‚é—®é¢˜ = Sub-query1 + Sub-query2 + ... + Sub-queryN

   å¹¶è¡Œæ£€ç´¢å„å­é—®é¢˜ â†’ èšåˆç»“æœ â†’ ç»¼åˆå›ç­”

   ä¾‹å¦‚ï¼š
   "æ¯”è¾ƒPythonå’ŒJavaçš„æ€§èƒ½ã€ç”Ÿæ€å’Œå­¦ä¹ æ›²çº¿"
   â†’ Sub1: "Pythonçš„æ€§èƒ½ç‰¹ç‚¹"
   â†’ Sub2: "Javaçš„æ€§èƒ½ç‰¹ç‚¹"
   â†’ Sub3: "Pythonå’ŒJavaçš„ç”Ÿæ€ç³»ç»Ÿå¯¹æ¯”"
   â†’ Sub4: "å­¦ä¹ æ›²çº¿å¯¹æ¯”"
   ```
4. **ç­–ç•¥é€‰æ‹©é€»è¾‘**ï¼š

   - **Rewrite**: æŸ¥è¯¢çŸ­ã€è¡¨è¾¾ä¸æ¸…ã€å…³é”®è¯ç¼ºå¤±
   - **Step-back**: å…·ä½“æŠ€æœ¯é—®é¢˜ã€éœ€è¦åŸç†æ€§çŸ¥è¯†
   - **Decompose**: åŒ…å«"å’Œ"ã€"å¯¹æ¯”"ã€"åˆ†æ"ç­‰å…³é”®è¯ï¼Œæ˜æ˜¾æ˜¯å¤åˆé—®é¢˜

#### âœ… ä¼˜ç‚¹

- **æå‡æ£€ç´¢è´¨é‡**ï¼šä¼˜åŒ–åçš„æŸ¥è¯¢æ›´å®¹æ˜“æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£
- **å¤šè§’åº¦è¦†ç›–**ï¼šç‰¹åˆ«æ˜¯Decomposeç­–ç•¥ï¼Œå…¨é¢è¦†ç›–é—®é¢˜
- **å¤„ç†å¤æ‚æŸ¥è¯¢**ï¼šèƒ½åº”å¯¹å¤šæ–¹é¢ã€å¤šå±‚æ¬¡çš„é—®é¢˜
- **çµæ´»æ€§å¼º**ï¼šä¸‰ç§ç­–ç•¥é€‚åº”ä¸åŒç±»å‹æŸ¥è¯¢
- **èƒŒæ™¯çŸ¥è¯†**ï¼šStep-backæä¾›æ›´æ·±å±‚ç†è§£

#### âŒ ç¼ºç‚¹

- **é¢å¤–LLMè°ƒç”¨**ï¼šæ¯ç§ç­–ç•¥éƒ½éœ€è¦é¢å¤–çš„LLMå¤„ç†
- **å»¶è¿Ÿå¢åŠ **ï¼šç‰¹åˆ«æ˜¯Decomposeéœ€è¦å¤šæ¬¡æ£€ç´¢
- **å¯èƒ½åç¦»**ï¼šè½¬æ¢åçš„æŸ¥è¯¢å¯èƒ½åç¦»åŸæ„
- **å¤æ‚åº¦é«˜**ï¼šéœ€è¦ç»´æŠ¤å¤šç§è½¬æ¢é€»è¾‘
- **æˆæœ¬å¢åŠ **ï¼šæ›´å¤šçš„LLMè°ƒç”¨å’Œæ£€ç´¢

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- ç”¨æˆ·æŸ¥è¯¢è¡¨è¾¾ä¸æ¸…æ™°
- å¤æ‚çš„å¤šæ–¹é¢é—®é¢˜
- éœ€è¦èƒŒæ™¯çŸ¥è¯†çš„æŠ€æœ¯é—®é¢˜
- å¯¹æ¯”åˆ†æç±»é—®é¢˜
- æ•™è‚²è¾…å¯¼ç³»ç»Ÿ
- ä¸“ä¸šå’¨è¯¢åŠ©æ‰‹

---

### 9. SemanticRag - è¯­ä¹‰åˆ†å—RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

SemanticRagä½¿ç”¨è¯­ä¹‰åˆ†å—æŠ€æœ¯ï¼Œä¸å†é‡‡ç”¨å›ºå®šå¤§å°åˆ†å—ï¼Œè€Œæ˜¯åŸºäºå¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦åŠ¨æ€ç¡®å®šåˆ†å—è¾¹ç•Œã€‚é€šè¿‡è®¡ç®—ç›¸é‚»å¥å­åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œåœ¨è¯­ä¹‰çªå˜ç‚¹ï¼ˆç›¸ä¼¼åº¦ä½çš„åœ°æ–¹ï¼‰è¿›è¡Œåˆ‡åˆ†ï¼Œä»è€Œä½¿æ¯ä¸ªchunkåœ¨è¯­ä¹‰ä¸Šæ›´åŠ è¿è´¯å®Œæ•´ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **è¯­ä¹‰è¿è´¯æ€§**ï¼šæŒ‰è¯­ä¹‰è‡ªç„¶è¾¹ç•Œåˆ†å—ï¼Œè€Œéæœºæ¢°çš„å­—æ•°åˆ‡å‰²
- **åŠ¨æ€æ–­ç‚¹**ï¼šåŸºäºå¥å­ç›¸ä¼¼åº¦æ›²çº¿æ‰¾å‡ºè¯­ä¹‰è·³å˜ä½ç½®
- **å®Œæ•´æ€§ä¿æŠ¤**ï¼šé¿å…åœ¨è¯­ä¹‰ç´§å¯†çš„å¥å­ä¸­é—´åˆ‡æ–­
- **æ™ºèƒ½åˆ†å‰²**ï¼šç»“åˆpercentileé˜ˆå€¼å’Œæœ€å°chunkå¤§å°æ§åˆ¶

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TB
    å¼€å§‹ --> è¾“å…¥æ–‡æœ¬
    è¾“å…¥æ–‡æœ¬ --> æŒ‰å¥å·åˆ‡åˆ†å¥å­
    æŒ‰å¥å·åˆ‡åˆ†å¥å­ --> ä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆå‘é‡
    ä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆå‘é‡ --> è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦
    è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦ --> æ‰¾å‡ºç›¸ä¼¼åº¦ä½çš„å¥å­ä½œä¸ºæ–­ç‚¹
    æ‰¾å‡ºç›¸ä¼¼åº¦ä½çš„å¥å­ä½œä¸ºæ–­ç‚¹ --> æ ¹æ®æ–­ç‚¹åˆ‡åˆ†æ–‡æœ¬å—
    æ ¹æ®æ–­ç‚¹åˆ‡åˆ†æ–‡æœ¬å— --> å­˜å‚¨æ–‡æœ¬å—
    å­˜å‚¨æ–‡æœ¬å— --> ç»“æŸ
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def _compute_breakpoints(self, sentences, embeddings):
    """è®¡ç®—è¯­ä¹‰æ–­ç‚¹"""
    if len(sentences) <= 1:
        return []

    # 1. è®¡ç®—ç›¸é‚»å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = []
    for i in range(len(embeddings) - 1):
        sim = self._cosine_similarity(embeddings[i], embeddings[i+1])
        similarities.append(sim)

    # 2. è½¬æ¢ä¸ºè¯­ä¹‰è·ç¦»ï¼ˆ1 - ç›¸ä¼¼åº¦ï¼‰
    distances = [1 - sim for sim in similarities]

    # 3. ä½¿ç”¨percentileç¡®å®šé˜ˆå€¼
    threshold = np.percentile(distances, self.breakpoint_percentile)

    # 4. æ‰¾å‡ºæ‰€æœ‰è¯­ä¹‰è·³å˜ç‚¹
    breakpoints = []
    current_chunk_size = 0

    for i, distance in enumerate(distances):
        current_chunk_size += len(sentences[i])

        # æ¡ä»¶ï¼šè·ç¦»è¶…è¿‡é˜ˆå€¼ ä¸” chunkå¤§å°è¶³å¤Ÿ
        if distance > threshold and current_chunk_size >= self.min_chunk_size:
            breakpoints.append(i + 1)
            current_chunk_size = 0

    return breakpoints

def _create_semantic_chunks(self, sentences, breakpoints):
    """æ ¹æ®æ–­ç‚¹åˆ›å»ºchunk"""
    if not breakpoints:
        return [". ".join(sentences)]

    chunks = []
    start = 0

    for bp in breakpoints:
        chunk_text = ". ".join(sentences[start:bp])
        chunks.append(chunk_text)
        start = bp

    # æœ€åä¸€ä¸ªchunk
    if start < len(sentences):
        chunk_text = ". ".join(sentences[start:])
        chunks.append(chunk_text)

    return chunks
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **å¥å­Embedding**ï¼š

   ```
   æ¯ä¸ªå¥å­ â†’ Embeddingå‘é‡ â†’ é«˜ç»´è¯­ä¹‰ç©ºé—´ä¸­çš„ç‚¹
   ```
2. **è¯­ä¹‰è·ç¦»è®¡ç®—**ï¼š

   ```
   ç›¸ä¼¼åº¦ = cosine_similarity(embedding_i, embedding_{i+1})
   è¯­ä¹‰è·ç¦» = 1 - ç›¸ä¼¼åº¦

   è·ç¦»å¤§ â†’ è¯­ä¹‰è·³å˜å¤§ â†’ å¯èƒ½æ˜¯åˆ†å—è¾¹ç•Œ
   è·ç¦»å° â†’ è¯­ä¹‰è¿ç»­ â†’ åº”ä¿æŒåœ¨åŒä¸€chunk
   ```
3. **åŠ¨æ€é˜ˆå€¼**ï¼š

   ```python
   threshold = np.percentile(distances, 80)  # ä½¿ç”¨80åˆ†ä½æ•°

   # è¿™æ„å‘³ç€åªæœ‰å‰20%æœ€å¤§çš„è¯­ä¹‰è·³å˜æ‰ä¼šè¢«æ ‡è®°ä¸ºæ–­ç‚¹
   ```
4. **æœ€å°chunkçº¦æŸ**ï¼š

   - é¿å…äº§ç”Ÿè¿‡å°çš„chunkï¼ˆä¿¡æ¯ä¸å®Œæ•´ï¼‰
   - å³ä½¿è¯­ä¹‰è·³å˜ï¼Œä¹Ÿéœ€æ»¡è¶³æœ€å°å¤§å°è¦æ±‚

#### âœ… ä¼˜ç‚¹

- **è¯­ä¹‰å®Œæ•´æ€§**ï¼šæ¯ä¸ªchunkåœ¨è¯­ä¹‰ä¸Šæ›´è¿è´¯
- **è‡ªç„¶è¾¹ç•Œ**ï¼šåœ¨ä¸»é¢˜æˆ–æ®µè½è½¬æ¢å¤„åˆ†å‰²
- **çµæ´»é€‚åº”**ï¼šè‡ªåŠ¨é€‚åº”æ–‡æ¡£çš„è¯­ä¹‰ç»“æ„
- **æ£€ç´¢ç²¾åº¦é«˜**ï¼šè¯­ä¹‰å®Œæ•´çš„chunkæ›´å®¹æ˜“åŒ¹é…æŸ¥è¯¢
- **å‡å°‘ä¿¡æ¯ç¢ç‰‡**ï¼šé¿å…åœ¨å¥å­ä¸­é—´æˆ–æ®µè½ä¸­é—´åˆ‡æ–­

#### âŒ ç¼ºç‚¹

- **è®¡ç®—å¼€é”€å¤§**ï¼šéœ€è¦ä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆembedding
- **å¤„ç†é€Ÿåº¦æ…¢**ï¼šç›¸ä¼¼åº¦è®¡ç®—çš„å¤æ‚åº¦ä¸ºO(n)
- **ä¸é€‚åˆé•¿å¥**ï¼šå¥å­æœ¬èº«å¾ˆé•¿æ—¶åˆ†å—ä»ç„¶å›°éš¾
- **ä¾èµ–å¥å­è´¨é‡**ï¼šæ ‡ç‚¹ç¬¦å·é”™è¯¯ä¼šå½±å“åˆ†å¥
- **å¯èƒ½ä¸å‡åŒ€**ï¼šæœ‰äº›chunkå¯èƒ½å¾ˆå¤§ï¼Œæœ‰äº›å¾ˆå°

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- é•¿æ–‡æ¡£æˆ–ä¹¦ç±ç« èŠ‚
- ä¸»é¢˜æ˜ç¡®ä¸”æœ‰æ¸…æ™°æ®µè½ç»“æ„çš„æ–‡æ¡£
- å­¦æœ¯è®ºæ–‡ï¼ˆæœ‰æ˜æ˜¾çš„ç« èŠ‚è½¬æ¢ï¼‰
- æ–°é—»æ–‡ç« ï¼ˆæ®µè½ä¸»é¢˜å˜åŒ–æ˜æ˜¾ï¼‰
- éœ€è¦ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„åº”ç”¨

---

### 10. HierarchyRAG - å±‚æ¬¡åŒ–æ£€ç´¢RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

HierarchyRAGå®ç°äº†ä¸¤çº§æ£€ç´¢ç­–ç•¥ï¼šç¬¬ä¸€çº§æœç´¢é¡µé¢/æ–‡æ¡£çº§åˆ«çš„æ‘˜è¦ï¼Œç¬¬äºŒçº§åœ¨æ‰¾åˆ°çš„ç›¸å…³é¡µé¢å†…æœç´¢è¯¦ç»†çš„æ–‡æœ¬å—ã€‚è¿™ç§å±‚æ¬¡åŒ–ç»“æ„æ¨¡æ‹Ÿäº†äººç±»æŸ¥æ‰¾ä¿¡æ¯çš„æ–¹å¼â€”â€”å…ˆæ‰¾åˆ°ç›¸å…³ç« èŠ‚ï¼Œå†åœ¨ç« èŠ‚å†…ç»†è¯»ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **ä¸¤çº§ç´¢å¼•**ï¼šç²—ç²’åº¦æ‘˜è¦ + ç»†ç²’åº¦chunks
- **å…ˆç²—åç»†**ï¼šå…ˆå®šä½å¤§è‡´èŒƒå›´ï¼Œå†ç²¾ç¡®æ£€ç´¢
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šchunksæºå¸¦å…¶æ‰€å±é¡µé¢çš„ä¿¡æ¯
- **åˆ†æ­¥è¿‡æ»¤**ï¼šå‡å°‘æ— å…³chunkçš„å¹²æ‰°

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TB
    å¼€å§‹ --> è¾“å…¥æ–‡æ¡£
    è¾“å…¥æ–‡æ¡£ --> ç”Ÿæˆæ¯é¡µæ‘˜è¦
    è¾“å…¥æ–‡æ¡£ --> å°†æ¯é¡µå†…å®¹åˆ†å—

    ç”¨æˆ·æŸ¥è¯¢ --> ç¬¬ä¸€æ­¥æœç´¢æ‘˜è¦
    ç¬¬ä¸€æ­¥æœç´¢æ‘˜è¦ --> æ‰¾åˆ°ç›¸å…³é¡µé¢
    æ‰¾åˆ°ç›¸å…³é¡µé¢ --> ç¬¬äºŒæ­¥åœ¨ç›¸å…³é¡µé¢å†…æœç´¢å†…å®¹å—
    ç¬¬äºŒæ­¥åœ¨ç›¸å…³é¡µé¢å†…æœç´¢å†…å®¹å— --> æ„å»ºä¸Šä¸‹æ–‡
    æ„å»ºä¸Šä¸‹æ–‡ --> ç”Ÿæˆç­”æ¡ˆ
    ç”Ÿæˆç­”æ¡ˆ --> ç»“æŸ
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def generate_page_summary(self, text, page_num):
    """ç”Ÿæˆé¡µé¢æ‘˜è¦"""
    # 1. ä½¿ç”¨LLMæ€»ç»“é¡µé¢å†…å®¹
    prompt = f"""
    è¯·ç”¨2-3å¥è¯æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®ä¿¡æ¯ï¼š

    {text[:2000]}  # å–å‰2000å­—ç¬¦
    """

    summary = self.llm_client.generate_text(
        prompt,
        system_instruction="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹"
    )

    # 2. ç”Ÿæˆæ‘˜è¦çš„embedding
    summary_embedding = self.embed_model.encode([summary])[0]

    # 3. å­˜å‚¨åˆ°summaries collection
    self.summaries_collection.insert([{
        "vector": summary_embedding,
        "page_num": page_num,
        "summary": summary,
        "text": text
    }])

    return summary

def hierarchical_search(self, query, top_k_pages=3, top_k_chunks=5):
    """å±‚æ¬¡åŒ–æœç´¢"""
    # ç¬¬1çº§ï¼šæœç´¢é¡µé¢æ‘˜è¦
    query_embedding = self.embed_model.encode([query])[0]

    summary_results = self.summaries_collection.search(
        data=[query_embedding],
        anns_field="vector",
        param={"metric_type": "COSINE"},
        limit=top_k_pages,
        output_fields=["page_num", "summary"]
    )[0]

    # æå–ç›¸å…³é¡µé¢ç¼–å·
    relevant_pages = [r.entity.get("page_num") for r in summary_results]

    print(f"ç¬¬1çº§ï¼šæ‰¾åˆ°ç›¸å…³é¡µé¢ {relevant_pages}")

    # ç¬¬2çº§ï¼šåœ¨ç›¸å…³é¡µé¢çš„chunksä¸­æœç´¢
    # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼
    page_filter = f"page_num in {relevant_pages}"

    chunk_results = self.chunks_collection.search(
        data=[query_embedding],
        anns_field="vector",
        param={"metric_type": "COSINE"},
        limit=top_k_chunks * len(relevant_pages),
        output_fields=["text", "page_num", "page_summary"],
        expr=page_filter  # åªæœç´¢ç›¸å…³é¡µé¢çš„chunks
    )[0]

    return chunk_results
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **ä¸¤çº§ç´¢å¼•ç»“æ„**ï¼š

   ```
   Level 1: é¡µé¢æ‘˜è¦ç´¢å¼•
   â”œâ”€ Summary 1 (Page 1-5) â†’ Embedding_1
   â”œâ”€ Summary 2 (Page 6-10) â†’ Embedding_2
   â””â”€ Summary 3 (Page 11-15) â†’ Embedding_3

   Level 2: Chunkè¯¦ç»†ç´¢å¼•
   â”œâ”€ Chunk 1.1 (Page 1) â†’ Embedding_1.1
   â”œâ”€ Chunk 1.2 (Page 1) â†’ Embedding_1.2
   â””â”€ ...
   ```
2. **æ£€ç´¢æµç¨‹**ï¼š

   ```
   Query â†’ Embed â†’ Search Level 1 â†’ Get Page IDs
                                    â†“
                          Filter Level 2 by Page IDs
                                    â†“
                            Search in Filtered Chunks
   ```
3. **ä¿¡æ¯å¢å¼º**ï¼š

   ```python
   chunk_data = {
       "text": chunk_text,           # chunkæœ¬èº«çš„æ–‡æœ¬
       "page_num": page_num,          # æ‰€å±é¡µé¢
       "page_summary": summary,       # é¡µé¢æ‘˜è¦ï¼ˆå¢å¼ºä¸Šä¸‹æ–‡ï¼‰
       "vector": chunk_embedding
   }
   ```
4. **å‡å°‘æœç´¢ç©ºé—´**ï¼š

   - ä¸æœç´¢ï¼šåœ¨100,000ä¸ªchunksä¸­ç›´æ¥æœç´¢
   - è€Œæ˜¯ï¼šå…ˆåœ¨1,000ä¸ªé¡µé¢æ‘˜è¦ä¸­æ‰¾3ä¸ª â†’ å†åœ¨300ä¸ªchunksä¸­æœç´¢
   - æœç´¢æ•ˆç‡æå‡æ˜¾è‘—

#### âœ… ä¼˜ç‚¹

- **æ£€ç´¢æ•ˆç‡é«˜**ï¼šä¸¤çº§è¿‡æ»¤å¤§å¹…å‡å°‘æœç´¢ç©ºé—´
- **å‡†ç¡®æ€§æå‡**ï¼šå…ˆå®šä½å¤§è‡´èŒƒå›´å†ç²¾ç¡®æœç´¢
- **ä¸Šä¸‹æ–‡ä¸°å¯Œ**ï¼šchunkæºå¸¦é¡µé¢æ‘˜è¦ä¿¡æ¯
- **å¯æ‰©å±•æ€§å¥½**ï¼šé€‚ç”¨äºå¤§è§„æ¨¡æ–‡æ¡£åº“
- **ç»“æ„æ¸…æ™°**ï¼šç¬¦åˆäººç±»æ£€ç´¢é€»è¾‘

#### âŒ ç¼ºç‚¹

- **ä¾èµ–æ‘˜è¦è´¨é‡**ï¼šç¬¬1çº§é”™è¯¯ä¼šå½±å“æ•´ä½“
- **è·¨é¡µé¢ä¿¡æ¯**ï¼šéš¾ä»¥å¤„ç†è·¨é¡µé¢çš„å…³è”ä¿¡æ¯
- **é¢å¤–LLMè°ƒç”¨**ï¼šç”Ÿæˆæ‘˜è¦éœ€è¦é¢å¤–æˆæœ¬
- **å­˜å‚¨å¼€é”€**ï¼šéœ€è¦ç»´æŠ¤ä¸¤ä¸ªcollection
- **é¡µé¢åˆ’åˆ†**ï¼šéœ€è¦åˆç†çš„é¡µé¢/ç« èŠ‚åˆ’åˆ†

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- å¤§å‹æ–‡æ¡£åº“æˆ–çŸ¥è¯†åº“
- ä¹¦ç±ã€æ‰‹å†Œã€æŠ€æœ¯æ–‡æ¡£
- æœ‰æ˜ç¡®ç« èŠ‚ç»“æ„çš„å†…å®¹
- å¤šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
- ä¼ä¸šå†…éƒ¨çŸ¥è¯†ç®¡ç†
- æ³•å¾‹æ–‡æ¡£æ£€ç´¢

---

### 11. ContextualCompressionRAG - ä¸Šä¸‹æ–‡å‹ç¼©RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

ContextualCompressionRAGä½¿ç”¨LLMå¯¹æ£€ç´¢åˆ°çš„chunksè¿›è¡Œæ™ºèƒ½å‹ç¼©å’Œè¿‡æ»¤ï¼Œåªä¿ç•™ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„éƒ¨åˆ†ã€‚é€šè¿‡ä¸‰ç§å‹ç¼©ç­–ç•¥ï¼ˆselectiveã€summaryã€extractionï¼‰ï¼Œå»é™¤æ— å…³ä¿¡æ¯ï¼Œå‡å°‘å™ªå£°ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **æ™ºèƒ½è¿‡æ»¤**ï¼šLLMåˆ¤æ–­å¹¶ä¿ç•™ç›¸å…³å†…å®¹
- **å™ªå£°æ¶ˆé™¤**ï¼šå»é™¤ä¸æŸ¥è¯¢æ— å…³çš„å¥å­å’Œæ®µè½
- **ä¸Šä¸‹æ–‡ç²¾ç®€**ï¼šå‡å°‘è¾“å…¥tokenï¼Œæé«˜æ•ˆç‡
- **ä¸‰ç§ç­–ç•¥**ï¼šé€‰æ‹©æ€§ä¿ç•™ã€æ‘˜è¦æå–ã€å…³é”®ä¿¡æ¯æŠ½å–

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
flowchart TB
    Start([å¼€å§‹]) --> Query[ç”¨æˆ·æŸ¥è¯¢]
    Query --> Search[å‘é‡æ£€ç´¢<br/>è·å–Top-K chunks]

    Search --> Strategy{é€‰æ‹©å‹ç¼©ç­–ç•¥}

    Strategy -->|selective| Selective[é€‰æ‹©æ€§å‹ç¼©<br/>ä¿ç•™ç›¸å…³å¥å­]
    Strategy -->|summary| Summary[æ‘˜è¦å‹ç¼©<br/>ç”Ÿæˆç®€çŸ­æ‘˜è¦]
    Strategy -->|extraction| Extract[æŠ½å–å‹ç¼©<br/>æå–å…³é”®ä¿¡æ¯]

    Selective --> AnalyzeSent[åˆ†ææ¯ä¸ªå¥å­]
    AnalyzeSent --> FilterSent[è¿‡æ»¤æ— å…³å¥å­]
    FilterSent --> CompressedChunk1[å‹ç¼©åçš„chunk]

    Summary --> GenSummary[LLMç”Ÿæˆæ‘˜è¦]
    GenSummary --> CompressedChunk2[å‹ç¼©åçš„chunk]

    Extract --> ExtractKey[LLMæŠ½å–å…³é”®ç‚¹]
    ExtractKey --> CompressedChunk3[å‹ç¼©åçš„chunk]

    CompressedChunk1 --> Merge[åˆå¹¶æ‰€æœ‰å‹ç¼©chunks]
    CompressedChunk2 --> Merge
    CompressedChunk3 --> Merge

    Merge --> Context[æ„å»ºç²¾ç®€ä¸Šä¸‹æ–‡]
    Context --> Generate[LLMç”Ÿæˆç­”æ¡ˆ]
    Generate --> End([ç»“æŸ])

    style Query fill:#E3F2FD
    style Search fill:#FFF9C4
    style Strategy fill:#F3E5F5
    style Selective fill:#E8F5E9
    style Summary fill:#FFE0B2
    style Extract fill:#DCEDC8
    style AnalyzeSent fill:#C5E1A5
    style GenSummary fill:#E1BEE7
    style ExtractKey fill:#FFCCBC
    style Merge fill:#FFF9C4
    style Generate fill:#F3E5F5
    style End fill:#FFE0B2
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def compress_chunk(self, chunk_text, query, compression_type="selective"):
    """å‹ç¼©å•ä¸ªchunk"""

    if compression_type == "selective":
        # é€‰æ‹©æ€§å‹ç¼©ï¼šä¿ç•™ç›¸å…³å¥å­
        prompt = f"""
        æŸ¥è¯¢ï¼š{query}

        æ–‡æœ¬ï¼š{chunk_text}

        è¯·åªä¿ç•™ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„å¥å­ï¼Œåˆ é™¤æ— å…³å†…å®¹ã€‚
        ä»¥JSONæ ¼å¼è¿”å›ï¼š{{"relevant_sentences": ["å¥å­1", "å¥å­2", ...]}}
        """

        response = self.llm_client.generate_text(prompt)
        relevant = json.loads(response)["relevant_sentences"]
        compressed = " ".join(relevant)

    elif compression_type == "summary":
        # æ‘˜è¦å‹ç¼©ï¼šç”Ÿæˆç®€çŸ­æ‘˜è¦
        prompt = f"""
        æŸ¥è¯¢ï¼š{query}

        æ–‡æœ¬ï¼š{chunk_text}

        è¯·é’ˆå¯¹æŸ¥è¯¢ï¼Œç”¨1-2å¥è¯æ€»ç»“æ–‡æœ¬ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚
        """

        compressed = self.llm_client.generate_text(prompt)

    elif compression_type == "extraction":
        # æŠ½å–å‹ç¼©ï¼šæå–å…³é”®ä¿¡æ¯ç‚¹
        prompt = f"""
        æŸ¥è¯¢ï¼š{query}

        æ–‡æœ¬ï¼š{chunk_text}

        è¯·æå–æ–‡æœ¬ä¸­ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ç‚¹ï¼Œç”¨ç®€æ´çš„è¦ç‚¹åˆ—å‡ºã€‚
        """

        compressed = self.llm_client.generate_text(prompt)

    return compressed
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **ä¸‰ç§å‹ç¼©ç­–ç•¥å¯¹æ¯”**ï¼š

   **Selectiveï¼ˆé€‰æ‹©æ€§ï¼‰**ï¼š

   ```
   åŸæ–‡ï¼šå¥å­Aï¼ˆç›¸å…³ï¼‰ã€‚å¥å­Bï¼ˆæ— å…³ï¼‰ã€‚å¥å­Cï¼ˆç›¸å…³ï¼‰ã€‚å¥å­Dï¼ˆæ— å…³ï¼‰ã€‚
   å‹ç¼©åï¼šå¥å­Aã€‚å¥å­Cã€‚
   ```

   **Summaryï¼ˆæ‘˜è¦ï¼‰**ï¼š

   ```
   åŸæ–‡ï¼šè¯¦ç»†æè¿°äº†Pythonçš„åˆ—è¡¨æ“ä½œï¼ŒåŒ…æ‹¬appendã€extendã€insertç­‰æ–¹æ³•...ï¼ˆ500å­—ï¼‰
   å‹ç¼©åï¼šPythonåˆ—è¡¨æ”¯æŒå¤šç§æ“ä½œæ–¹æ³•å¦‚appendå’Œextendã€‚
   ```

   **Extractionï¼ˆæŠ½å–ï¼‰**ï¼š

   ```
   åŸæ–‡ï¼šæ··æ‚çš„æ®µè½...
   å‹ç¼©åï¼š
   - è¦ç‚¹1ï¼šxxx
   - è¦ç‚¹2ï¼šyyy
   - è¦ç‚¹3ï¼šzzz
   ```
2. **TokenèŠ‚çœ**ï¼š

   ```
   åŸå§‹Top-10 chunks: 5000 tokens
   å‹ç¼©å: 1500 tokens

   èŠ‚çœ60%ï¼Œä½†ä¿ç•™äº†æ ¸å¿ƒä¿¡æ¯
   ```

#### âœ… ä¼˜ç‚¹

- **å‡å°‘å™ªå£°**ï¼šè¿‡æ»¤æ— å…³ä¿¡æ¯ï¼Œæé«˜ç­”æ¡ˆè´¨é‡
- **èŠ‚çœToken**ï¼šå‡å°‘è¾“å…¥é•¿åº¦ï¼Œé™ä½æˆæœ¬
- **æå‡ç²¾åº¦**ï¼šLLMèƒ½æ›´èšç„¦äºç›¸å…³ä¿¡æ¯
- **çµæ´»ç­–ç•¥**ï¼šä¸‰ç§å‹ç¼©æ–¹å¼é€‚åº”ä¸åŒéœ€æ±‚

#### âŒ ç¼ºç‚¹

- **é¢å¤–LLMè°ƒç”¨**ï¼šæ¯ä¸ªchunkéƒ½éœ€è¦å‹ç¼©å¤„ç†
- **å»¶è¿Ÿå¢åŠ **ï¼šå¤šæ¬¡LLMè°ƒç”¨å¯¼è‡´å“åº”å˜æ…¢
- **æˆæœ¬é«˜æ˜‚**ï¼šå¤§é‡chunkå‹ç¼©ä¼šæ˜¾è‘—å¢åŠ æˆæœ¬
- **ä¿¡æ¯ä¸¢å¤±é£é™©**ï¼šå‹ç¼©å¯èƒ½åˆ é™¤æœ‰ç”¨ä¿¡æ¯

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- Chunkæ™®éè¾ƒé•¿ä¸”åŒ…å«å¤§é‡æ— å…³ä¿¡æ¯
- å¯¹ç­”æ¡ˆç²¾åº¦è¦æ±‚æé«˜çš„åœºæ™¯
- Tokenæˆæœ¬æ•æ„Ÿçš„åº”ç”¨
- æŠ€æœ¯æ–‡æ¡£é—®ç­”
- æ³•å¾‹æ–‡æ¡£åˆ†æ

---

### 12. ContextEnrichedRAG - ä¸Šä¸‹æ–‡å¢å¼ºRAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

ContextEnrichedRAGåœ¨æ£€ç´¢åˆ°æœ€ç›¸å…³çš„chunkåï¼Œä¼šè‡ªåŠ¨è·å–è¯¥chunkå‰åç›¸é‚»çš„chunksï¼Œå°†å®ƒä»¬ä¸€èµ·ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMã€‚è¿™ç§æ–¹å¼å¢å¼ºäº†ä¸Šä¸‹æ–‡çš„è¿è´¯æ€§å’Œå®Œæ•´æ€§ï¼Œé¿å…ä¿¡æ¯è¢«æˆªæ–­ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **ä¸Šä¸‹æ–‡è¿ç»­æ€§**ï¼šä¸åªçœ‹å­¤ç«‹çš„chunkï¼Œè¿˜çœ‹å‰åæ–‡
- **ä¿¡æ¯å®Œæ•´æ€§**ï¼šé¿å…å…³é”®ä¿¡æ¯æ°å¥½è¢«åˆ‡å‰²åœ¨chunkè¾¹ç•Œ
- **çª—å£æ‰©å±•**ï¼šä»¥åŒ¹é…chunkä¸ºä¸­å¿ƒå‘ä¸¤è¾¹æ‰©å±•
- **è‡ªç„¶è¿‡æ¸¡**ï¼šæä¾›æ›´è‡ªç„¶çš„é˜…è¯»ä½“éªŒ

#### ğŸ”„ è¯¦ç»†æµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹] --> B{æ–‡æ¡£åˆ†å—å¹¶æ·»åŠ ç´¢å¼•};
    B --> C[å­˜å‚¨è‡³å‘é‡æ•°æ®åº“];
    C --> D{ç”¨æˆ·æé—®};
    D --> E[æ£€ç´¢ç›¸å…³æ–‡æœ¬å—];
    E --> F[æ‰©å±•ä¸Šä¸‹æ–‡<br>è·å–ç›¸é‚»æ–‡æœ¬å—];
    F --> G[æ•´åˆå¹¶æ’åºæ–‡æœ¬å—];
    G --> H[æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡];
    H --> I[å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ];
    I --> J[ç»“æŸ];
```

#### ğŸ’» å…³é”®ä»£ç å®ç°

```python
def context_enriched_search(self, query, top_k=3, context_window=1):
    """ä¸Šä¸‹æ–‡å¢å¼ºæ£€ç´¢"""
    # 1. æ ‡å‡†æ£€ç´¢
    query_embedding = self.embed_model.encode([query])[0]

    results = self.collection.search(
        data=[query_embedding],
        anns_field="vector",
        param={"metric_type": "COSINE"},
        limit=top_k,
        output_fields=["text", "chunk_index", "total_chunks", "file_path"]
    )[0]

    # 2. ä¸ºæ¯ä¸ªåŒ¹é…chunkæ”¶é›†ä¸Šä¸‹æ–‡
    enriched_chunks = []
    seen_indices = set()

    for result in results:
        chunk_index = result.entity.get("chunk_index")

        # è®¡ç®—æ‰©å±•èŒƒå›´
        start_index = max(0, chunk_index - context_window)
        end_index = min(total_chunks - 1, chunk_index + context_window)

        # è·å–æ‰©å±•èŒƒå›´å†…çš„æ‰€æœ‰chunks
        for idx in range(start_index, end_index + 1):
            if idx not in seen_indices:
                seen_indices.add(idx)
                # æŸ¥è¯¢ç‰¹å®šç´¢å¼•çš„chunk
                chunk_result = self.collection.query(
                    expr=f"chunk_index == {idx}",
                    output_fields=["text", "chunk_index"]
                )
                if chunk_result:
                    enriched_chunks.append({
                        "text": chunk_result[0]["text"],
                        "index": idx,
                        "is_matched": (idx == chunk_index)
                    })

    # 3. æŒ‰åŸæ–‡é¡ºåºæ’åº
    enriched_chunks.sort(key=lambda x: x["index"])
    return enriched_chunks
```

#### ğŸ”¬ ç®—æ³•åŸç†

1. **ä¸Šä¸‹æ–‡çª—å£æœºåˆ¶**ï¼š

   ```
   æ–‡æ¡£åˆ‡åˆ†ï¼š[Chunk0] [Chunk1] [Chunk2] [Chunk3] [Chunk4] [Chunk5]

   æ£€ç´¢ç»“æœï¼šChunk2 æœ€åŒ¹é…

   Context Window = 1:
   è¿”å›ï¼š[Chunk1] [Chunk2*] [Chunk3]

   Context Window = 2:
   è¿”å›ï¼š[Chunk0] [Chunk1] [Chunk2*] [Chunk3] [Chunk4]
   ```
2. **å®Œæ•´æ€§ä¿éšœ**ï¼š
   é¿å…å…³é”®ä¿¡æ¯è¢«chunkè¾¹ç•Œåˆ‡æ–­ï¼Œæä¾›å®Œæ•´ä¸Šä¸‹æ–‡ã€‚

#### âœ… ä¼˜ç‚¹

- **ä¸Šä¸‹æ–‡è¿è´¯**ï¼šæä¾›æ›´è‡ªç„¶ã€å®Œæ•´çš„é˜…è¯»ä½“éªŒ
- **ä¿¡æ¯å®Œæ•´æ€§**ï¼šé¿å…å…³é”®ä¿¡æ¯è¢«chunkè¾¹ç•Œåˆ‡æ–­
- **å®ç°ç®€å•**ï¼šåªéœ€æ·»åŠ ç´¢å¼•å’Œæ‰©å±•é€»è¾‘
- **çµæ´»è°ƒèŠ‚**ï¼šå¯è°ƒæ•´context_windowå¤§å°

#### âŒ ç¼ºç‚¹

- **å¯èƒ½å¼•å…¥å™ªå£°**ï¼šç›¸é‚»chunkså¯èƒ½åŒ…å«æ— å…³ä¿¡æ¯
- **Tokenå¢åŠ **ï¼šæ‰©å±•ä¸Šä¸‹æ–‡ä¼šå¢åŠ è¾“å…¥é•¿åº¦
- **ä¸¥æ ¼é¡ºåºä¾èµ–**ï¼šè¦æ±‚chunksæŒ‰åŸæ–‡é¡ºåºå­˜å‚¨
- **è·¨æ–‡æ¡£é™åˆ¶**ï¼šä¸é€‚ç”¨äºå¤šæ–‡æ¡£æ··åˆæ£€ç´¢

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- é•¿æ–‡æ¡£æˆ–è¿ç»­æ€§å¼ºçš„æ–‡æœ¬
- æŠ€æœ¯æ•™ç¨‹å’Œæ–‡æ¡£ï¼ˆæ­¥éª¤è¿ç»­ï¼‰
- å°è¯´ã€æ•…äº‹ï¼ˆæƒ…èŠ‚è¿è´¯ï¼‰
- ä»£ç æ–‡æ¡£ï¼ˆä¸Šä¸‹æ–‡ä¾èµ–ï¼‰
- ä¼šè®®è®°å½•å’ŒæŠ¥å‘Š

---

### 13. ContextualChunkProcessor - ä¸Šä¸‹æ–‡æ ‡é¢˜RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

ContextualChunkProcessorä¸ºæ¯ä¸ªchunkç”Ÿæˆä¸€ä¸ªæè¿°æ€§çš„æ ‡é¢˜ï¼ˆheaderï¼‰ï¼Œè¯¥æ ‡é¢˜æ¦‚æ‹¬äº†chunkçš„ä¸»é¢˜å’Œå†…å®¹ã€‚åœ¨æ£€ç´¢æ—¶ï¼Œä½¿ç”¨åŒå‘é‡ç­–ç•¥ï¼šæ—¢æœç´¢chunkæ–‡æœ¬çš„embeddingï¼Œä¹Ÿæœç´¢headerçš„embeddingï¼Œä»è€Œæå‡æ£€ç´¢çš„ä¸»é¢˜åŒ¹é…åº¦ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **åŒå‘é‡ç´¢å¼•**ï¼štext_vector + header_vector
- **ä¸»é¢˜å¢å¼º**ï¼šheaderæ˜ç¡®chunkçš„ä¸»é¢˜
- **åŒé‡åŒ¹é…**ï¼šæ–‡æœ¬åŒ¹é… + ä¸»é¢˜åŒ¹é…

#### âœ… ä¼˜ç‚¹

- **ä¸»é¢˜åŒ¹é…å¢å¼º**ï¼šä¸åªé å…³é”®è¯ï¼Œè¿˜è€ƒè™‘ä¸»é¢˜ç›¸å…³æ€§
- **æ£€ç´¢å¬å›ç‡é«˜**ï¼šåŒå‘é‡å¢åŠ åŒ¹é…æœºä¼š
- **è¯­ä¹‰ç†è§£æ›´æ·±**ï¼šheaderæä¾›æŠ½è±¡å±‚é¢çš„ç†è§£

#### âŒ ç¼ºç‚¹

- **Headerç”Ÿæˆæˆæœ¬**ï¼šæ¯ä¸ªchunkéƒ½éœ€è¦LLMè°ƒç”¨
- **å­˜å‚¨å¼€é”€**ï¼šéœ€è¦å­˜å‚¨ä¸¤ä¸ªå‘é‡å­—æ®µ
- **æ£€ç´¢å¤æ‚åº¦**ï¼šéœ€è¦æ‰§è¡Œä¸¤æ¬¡å‘é‡æœç´¢

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- æŠ€æœ¯æ–‡æ¡£ï¼ˆç« èŠ‚ä¸»é¢˜æ˜ç¡®ï¼‰
- æ–°é—»æ–‡ç« æ£€ç´¢
- å­¦æœ¯è®ºæ–‡æ£€ç´¢
- äº§å“æ‰‹å†Œå’ŒFAQ

---

### 14. DocumentAugmentationRAG - æ–‡æ¡£å¢å¼ºRAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

DocumentAugmentationRAGä¸ºæ¯ä¸ªchunkç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜ï¼Œå¹¶å°†è¿™äº›é—®é¢˜ä¸chunkä¸€èµ·ç´¢å¼•ã€‚æ£€ç´¢æ—¶ï¼ŒåŒæ—¶æœç´¢chunkæ–‡æœ¬å’Œç”Ÿæˆçš„é—®é¢˜ï¼Œä»è€Œæé«˜é—®ç­”åœºæ™¯ä¸‹çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **åå‘é—®é¢˜ç”Ÿæˆ**ï¼šä»ç­”æ¡ˆï¼ˆchunkï¼‰ç”Ÿæˆé—®é¢˜
- **é—®é¢˜ç´¢å¼•**ï¼šå°†ç”Ÿæˆçš„é—®é¢˜ä¹Ÿçº³å…¥æ£€ç´¢èŒƒå›´
- **é—®ç­”å¯¹é½**ï¼šç”¨æˆ·é—®é¢˜æ›´å®¹æ˜“åŒ¹é…ç”Ÿæˆçš„é—®é¢˜
- **å¤šè§’åº¦è¦†ç›–**ï¼šä¸€ä¸ªchunkå¯¹åº”å¤šä¸ªæ½œåœ¨é—®é¢˜

#### âœ… ä¼˜ç‚¹

- **é—®ç­”å¯¹é½**ï¼šç”¨æˆ·é—®é¢˜ä¸ç”Ÿæˆé—®é¢˜é«˜åº¦å¯¹é½
- **å¬å›ç‡æå‡**ï¼šå¤šä¸ªé—®é¢˜è§’åº¦å¢åŠ åŒ¹é…æœºä¼š
- **è¯­ä¹‰æ³›åŒ–**ï¼šä¸åŒè¡¨è¾¾æ–¹å¼çš„é—®é¢˜éƒ½èƒ½åŒ¹é…
- **é€‚åˆFAQ**ï¼šå¤©ç„¶é€‚åˆé—®ç­”åœºæ™¯

#### âŒ ç¼ºç‚¹

- **ç”Ÿæˆæˆæœ¬é«˜**ï¼šæ¯ä¸ªchunkéƒ½éœ€è¦ç”Ÿæˆé—®é¢˜
- **é—®é¢˜è´¨é‡ä¾èµ–**ï¼šä¾èµ–LLMç”Ÿæˆé«˜è´¨é‡é—®é¢˜
- **å­˜å‚¨å¼€é”€å¤§**ï¼šéœ€è¦å­˜å‚¨é¢å¤–çš„é—®é¢˜æ•°æ®

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- FAQç³»ç»Ÿã€å®¢æœæœºå™¨äººã€åœ¨çº¿å¸®åŠ©æ–‡æ¡£
- æŠ€æœ¯æ”¯æŒç³»ç»Ÿã€æ•™è‚²é—®ç­”å¹³å°

---

### 15. FeedbackLoopRAG - åé¦ˆå¾ªç¯RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

FeedbackLoopRAGå¼•å…¥ç”¨æˆ·åé¦ˆæœºåˆ¶ï¼Œæ ¹æ®ç”¨æˆ·å¯¹æ£€ç´¢ç»“æœçš„è¯„ä»·ï¼ˆæœ‰ç”¨/æ— ç”¨ï¼‰åŠ¨æ€è°ƒæ•´ç›¸å…³æ€§åˆ†æ•°ã€‚é€šè¿‡æŒç»­å­¦ä¹ ç”¨æˆ·åå¥½ï¼Œé€æ­¥æå‡æ£€ç´¢è´¨é‡ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **ç”¨æˆ·åé¦ˆ**ï¼šæ”¶é›†ç”¨æˆ·å¯¹æ£€ç´¢ç»“æœçš„è¯„ä»·
- **åŠ¨æ€è°ƒæ•´**ï¼šæ ¹æ®åé¦ˆè°ƒæ•´chunkçš„ç›¸å…³æ€§åˆ†æ•°
- **æŒç»­å­¦ä¹ **ï¼šç´¯ç§¯åé¦ˆæ•°æ®ä¼˜åŒ–æ£€ç´¢ç­–ç•¥
- **ä¸ªæ€§åŒ–**ï¼šå­¦ä¹ ç‰¹å®šç”¨æˆ·æˆ–åœºæ™¯çš„åå¥½

#### âœ… ä¼˜ç‚¹

- **æŒç»­æ”¹è¿›**ï¼šéšç€ä½¿ç”¨ä¸æ–­ä¼˜åŒ–
- **ä¸ªæ€§åŒ–**ï¼šå­¦ä¹ ç‰¹å®šåœºæ™¯çš„åå¥½
- **ç®€å•æœ‰æ•ˆ**ï¼šå®ç°ç®€å•ä½†æ•ˆæœæ˜æ˜¾
- **ç”¨æˆ·å‚ä¸**ï¼šå¢å¼ºç”¨æˆ·å‚ä¸æ„Ÿ

#### âŒ ç¼ºç‚¹

- **å†·å¯åŠ¨é—®é¢˜**ï¼šåˆæœŸæ— åé¦ˆæ•°æ®
- **åé¦ˆç¨€ç–**ï¼šç”¨æˆ·ä¸ä¸€å®šæ„¿æ„æä¾›åé¦ˆ
- **åé¦ˆåå·®**ï¼šå¯èƒ½å­˜åœ¨ç”¨æˆ·åè§

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- ä¼ä¸šå†…éƒ¨çŸ¥è¯†åº“ï¼ˆç”¨æˆ·ç¾¤ä½“å›ºå®šï¼‰
- å®¢æœç³»ç»Ÿï¼ˆé«˜é¢‘ä½¿ç”¨åœºæ™¯ï¼‰
- ä¸“ä¸šé¢†åŸŸé—®ç­”ï¼ˆä¸“å®¶åé¦ˆï¼‰
- é•¿æœŸè¿è¥çš„RAGç³»ç»Ÿ

---

### 16. RSERAG - ç›¸å…³æ®µè½æå–RAG

#### ğŸ“– æ–¹æ³•ç®€ä»‹

RSERAGä½¿ç”¨æœ€å¤§å­æ•°ç»„ç®—æ³•ï¼ˆç±»ä¼¼è‚¡ç¥¨ä¹°å–é—®é¢˜ï¼‰æ¥æ‰¾åˆ°æ–‡æ¡£ä¸­æœ€ç›¸å…³çš„è¿ç»­æ®µè½ã€‚ä¸æ˜¯å•ç‹¬è¯„ä¼°æ¯ä¸ªchunkï¼Œè€Œæ˜¯å¯»æ‰¾"ç´¯ç§¯ç›¸å…³æ€§"æœ€é«˜çš„è¿ç»­chunkåºåˆ—ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯ä¸”é«˜åº¦ç›¸å…³ã€‚

#### ğŸ¯ æ ¸å¿ƒæ€æƒ³

- **è¿ç»­æ€§ä¼˜åŒ–**ï¼šå¯»æ‰¾æœ€ä¼˜çš„è¿ç»­chunkåºåˆ—
- **ç´¯ç§¯ç›¸å…³æ€§**ï¼šè€ƒè™‘chunkåºåˆ—çš„æ•´ä½“ä»·å€¼
- **åŠ¨æ€è§„åˆ’**ï¼šä½¿ç”¨Kadaneç®—æ³•å˜ä½“
- **ä¸Šä¸‹æ–‡å®Œæ•´**ï¼šè‡ªç„¶ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§

#### ğŸ”¬ ç®—æ³•åŸç†

ä½¿ç”¨æœ€å¤§å­æ•°ç»„ç®—æ³•ï¼ˆKadane's Algorithmï¼‰ï¼š

#### âœ… ä¼˜ç‚¹

- **è¿ç»­æ€§ä¿è¯**ï¼šè‡ªåŠ¨ä¿æŒchunkçš„è¿è´¯æ€§
- **æ•´ä½“ä¼˜åŒ–**ï¼šå…¨å±€æœ€ä¼˜æ®µè½è€Œéå±€éƒ¨æœ€ä¼˜
- **ç®—æ³•é«˜æ•ˆ**ï¼šO(n)æ—¶é—´å¤æ‚åº¦
- **ä¸Šä¸‹æ–‡å®Œæ•´**ï¼šå¤©ç„¶åŒ…å«å‰åæ–‡ä¿¡æ¯
- **è‡ªé€‚åº”é•¿åº¦**ï¼šæ ¹æ®ç›¸å…³æ€§åŠ¨æ€ç¡®å®šæ®µè½é•¿åº¦

#### âŒ ç¼ºç‚¹

- **ä¸¥æ ¼é¡ºåºä¾èµ–**ï¼šè¦æ±‚chunksä¸¥æ ¼æŒ‰åŸæ–‡é¡ºåº
- **é˜ˆå€¼æ•æ„Ÿ**ï¼šthresholdå‚æ•°éœ€è¦ä»”ç»†è°ƒä¼˜
- **å•æ®µè½é™åˆ¶**ï¼šåªè¿”å›ä¸€ä¸ªæœ€ä½³æ®µè½
- **ä¸é€‚åˆè·³è·ƒå¼ä¿¡æ¯**ï¼šç›¸å…³ä¿¡æ¯åˆ†æ•£æ—¶æ•ˆæœå·®

#### ğŸ¯ é€‚ç”¨åœºæ™¯

- é•¿æ–‡æ¡£é˜…è¯»ç†è§£
- è¿ç»­æ€§å¼ºçš„æ–‡æœ¬ï¼ˆå°è¯´ã€æŠ¥å‘Šï¼‰
- éœ€è¦å®Œæ•´æ®µè½çš„åº”ç”¨
- æŠ€æœ¯æ–‡æ¡£ï¼ˆæ­¥éª¤è¿ç»­ï¼‰
- é¿å…ç¢ç‰‡åŒ–ä¿¡æ¯çš„åœºæ™¯

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯¹æ¯”åˆ†æä¸é€‰æ‹©æŒ‡å—

### ğŸ¯ æŠ€æœ¯ç‰¹æ€§å¯¹æ¯”

#### åˆ†å—ç­–ç•¥åˆ†ç±»

| ç­–ç•¥ç±»å‹             | æ–¹æ³•                                                      | ç‰¹ç‚¹                           |
| -------------------- | --------------------------------------------------------- | ------------------------------ |
| **å›ºå®šåˆ†å—**   | SimpleRAG, AdaptiveRAG, HyDERAG, CRAG, SelfRAG, RerankRAG | ç®€å•é«˜æ•ˆï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯       |
| **è¯­ä¹‰åˆ†å—**   | SemanticRag                                               | ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œé€‚åˆç»“æ„åŒ–æ–‡æ¡£ |
| **å±‚æ¬¡åˆ†å—**   | HierarchyRAG                                              | å¤šçº§æ£€ç´¢ï¼Œé€‚åˆå¤§è§„æ¨¡æ–‡æ¡£       |
| **ä¸Šä¸‹æ–‡æ‰©å±•** | ContextEnrichedRAG, RSERAG                                | åŒ…å«å‰åæ–‡ï¼Œé€‚åˆè¿ç»­æ–‡æœ¬       |

#### æ£€ç´¢ç­–ç•¥åˆ†ç±»

| ç­–ç•¥ç±»å‹             | æ–¹æ³•                            | ä¼˜åŠ¿                        |
| -------------------- | ------------------------------- | --------------------------- |
| **å•å‘é‡æ£€ç´¢** | SimpleRAG, AdaptiveRAG, HyDERAG | é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½              |
| **åŒå‘é‡æ£€ç´¢** | ContextualChunkProcessor        | æ–‡æœ¬+ä¸»é¢˜åŒé‡åŒ¹é…           |
| **æ··åˆæ£€ç´¢**   | FusionRAG                       | BM25+å‘é‡ï¼Œè¦†ç›–å…³é”®è¯å’Œè¯­ä¹‰ |
| **é‡æ’åº**     | RerankRAG                       | ä¸¤é˜¶æ®µæ£€ç´¢ï¼Œç²¾åº¦æå‡        |
| **å±‚æ¬¡æ£€ç´¢**   | HierarchyRAG                    | å…ˆç²—åç»†ï¼Œæ•ˆç‡é«˜            |

#### ä¼˜åŒ–ç­–ç•¥åˆ†ç±»

| ç­–ç•¥ç±»å‹             | æ–¹æ³•                                         | æ ¸å¿ƒæŠ€æœ¯             |
| -------------------- | -------------------------------------------- | -------------------- |
| **æŸ¥è¯¢ä¼˜åŒ–**   | HyDERAG, QueryTransformRAG                   | æŸ¥è¯¢é‡å†™ã€åˆ†è§£ã€æ‰©å±• |
| **ç»“æœä¼˜åŒ–**   | CRAG, SelfRAG                                | ç›¸å…³æ€§è¯„ä¼°ã€è‡ªæˆ‘åæ€ |
| **ä¸Šä¸‹æ–‡ä¼˜åŒ–** | ContextualCompressionRAG, ContextEnrichedRAG | å‹ç¼©ã€æ‰©å±•           |
| **ç´¢å¼•ä¼˜åŒ–**   | DocumentAugmentationRAG                      | ç”Ÿæˆé—®é¢˜å¢å¼ºç´¢å¼•     |
| **åé¦ˆä¼˜åŒ–**   | FeedbackLoopRAG                              | ç”¨æˆ·åé¦ˆæŒç»­æ”¹è¿›     |

### ğŸ” åœºæ™¯é€‰æ‹©å†³ç­–æ ‘

```mermaid
flowchart TD
    Start([é€‰æ‹©RAGæ–¹æ³•]) --> Q1{é¡¹ç›®è§„æ¨¡?}

    Q1 -->|å°å‹è¯•éªŒ| Simple[SimpleRAG<br/>å¿«é€Ÿä¸Šæ‰‹]
    Q1 -->|ä¸­å¤§å‹| Q2{ä¸»è¦éœ€æ±‚?}

    Q2 -->|é«˜ç²¾åº¦| Q3{å¯æ¥å—é«˜æˆæœ¬?}
    Q3 -->|æ˜¯| Self[SelfRAG<br/>æœ€é«˜ç²¾åº¦]
    Q3 -->|å¦| Rerank[RerankRAG<br/>ç²¾åº¦å¹³è¡¡æˆæœ¬]

    Q2 -->|é«˜æ€§èƒ½| Q4{æ–‡æ¡£ç±»å‹?}
    Q4 -->|å¤§è§„æ¨¡æ–‡æ¡£åº“| Hierarchy[HierarchyRAG<br/>å±‚æ¬¡æ£€ç´¢]
    Q4 -->|ä¸€èˆ¬è§„æ¨¡| Fusion[FusionRAG<br/>æ··åˆæ£€ç´¢]

    Q2 -->|é—®ç­”åœºæ™¯| DocAug[DocumentAugmentationRAG<br/>é—®é¢˜å¢å¼º]

    Q2 -->|è¿ç»­æ–‡æœ¬| Q5{éœ€è¦å®Œæ•´æ®µè½?}
    Q5 -->|æ˜¯| RSE[RSERAG<br/>è¿ç»­æ®µè½æå–]
    Q5 -->|å¦| ContextEnrich[ContextEnrichedRAG<br/>ä¸Šä¸‹æ–‡æ‰©å±•]

    Q2 -->|é•¿æœŸè¿è¥| Feedback[FeedbackLoopRAG<br/>åé¦ˆä¼˜åŒ–]

    Q2 -->|å¤æ‚æŸ¥è¯¢| QueryTrans[QueryTransformRAG<br/>æŸ¥è¯¢è½¬æ¢]

    style Simple fill:#C8E6C9
    style Self fill:#FFCCBC
    style Rerank fill:#FFF9C4
    style Hierarchy fill:#E1BEE7
    style Fusion fill:#DCEDC8
    style DocAug fill:#FFE0B2
    style RSE fill:#F3E5F5
    style ContextEnrich fill:#E8F5E9
    style Feedback fill:#C5E1A5
    style QueryTrans fill:#FFE0B2
```

### ğŸ’¡ ç»„åˆä½¿ç”¨å»ºè®®

å¾ˆå¤šRAGæ–¹æ³•å¯ä»¥ç»„åˆä½¿ç”¨ï¼Œå‘æŒ¥ååŒæ•ˆåº”ï¼š

1. **é«˜ç²¾åº¦ç»„åˆ**ï¼š

   - FusionRAG (æ··åˆæ£€ç´¢) + RerankRAG (é‡æ’åº) + ContextualCompressionRAG (å‹ç¼©)
   - é€‚åˆï¼šå¯¹ç­”æ¡ˆè´¨é‡è¦æ±‚æé«˜çš„åœºæ™¯
2. **é«˜æ€§èƒ½ç»„åˆ**ï¼š

   - HierarchyRAG (å±‚æ¬¡æ£€ç´¢) + ContextEnrichedRAG (ä¸Šä¸‹æ–‡æ‰©å±•)
   - é€‚åˆï¼šå¤§è§„æ¨¡æ–‡æ¡£åº“çš„å¿«é€Ÿæ£€ç´¢
3. **å¹³è¡¡ç»„åˆ**ï¼š

   - AdaptiveRAG (è‡ªé€‚åº”åˆ†ç±») + RerankRAG (é‡æ’åº)
   - é€‚åˆï¼šå¤šæ ·åŒ–æŸ¥è¯¢ç±»å‹çš„é€šç”¨åœºæ™¯
4. **FAQä¸“ç”¨ç»„åˆ**ï¼š

   - DocumentAugmentationRAG (é—®é¢˜ç”Ÿæˆ) + FeedbackLoopRAG (åé¦ˆä¼˜åŒ–)
   - é€‚åˆï¼šå®¢æœç³»ç»Ÿå’ŒFAQåœºæ™¯
5. **é•¿æ–‡æœ¬ç»„åˆ**ï¼š

   - SemanticRag (è¯­ä¹‰åˆ†å—) + RSERAG (æ®µè½æå–) + ContextEnrichedRAG (ä¸Šä¸‹æ–‡æ‰©å±•)
   - é€‚åˆï¼šä¹¦ç±ã€é•¿æŠ¥å‘Šç­‰è¿ç»­æ–‡æœ¬

---

## ç¬¬å››éƒ¨åˆ†ï¼šå®è·µå»ºè®®

### ğŸš€ å¿«é€Ÿå¼€å§‹æŒ‡å—

#### æ–°æ‰‹è·¯å¾„ï¼ˆ0-1ä¸ªæœˆï¼‰

1. **ç¬¬1å‘¨ï¼šåŸºç¡€å…¥é—¨**

   - å®ç°å¹¶ç†è§£ SimpleRAG
   - æŒæ¡å‘é‡æ•°æ®åº“åŸºæœ¬æ“ä½œï¼ˆMilvus/Pineconeï¼‰
   - ç†è§£EmbeddingåŸç†å’Œåº”ç”¨
2. **ç¬¬2å‘¨ï¼šä¼˜åŒ–å°è¯•**

   - å°è¯• RerankRAG æˆ– FusionRAG
   - å­¦ä¹ è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼‰
   - å¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœ
3. **ç¬¬3-4å‘¨ï¼šé«˜çº§æ–¹æ³•**

   - æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©1-2ä¸ªé«˜çº§æ–¹æ³•å®è·µ
   - å­¦ä¹ å‚æ•°è°ƒä¼˜ï¼ˆchunk_size, top_k, temperatureï¼‰
   - å»ºç«‹è‡ªå·±çš„è¯„ä¼°æµ‹è¯•é›†

#### è¿›é˜¶è·¯å¾„ï¼ˆ1-3ä¸ªæœˆï¼‰

1. **æ·±å…¥ç†è§£**

   - ç ”ç©¶ SelfRAG å’Œ CRAG çš„è¯„ä¼°æœºåˆ¶
   - å­¦ä¹ æŸ¥è¯¢è½¬æ¢æŠ€æœ¯ï¼ˆHyDE, Query Rewritingï¼‰
   - æŒæ¡æ··åˆæ£€ç´¢ç­–ç•¥
2. **æ€§èƒ½ä¼˜åŒ–**

   - å­¦ä¹ ç¼“å­˜ç­–ç•¥å‡å°‘LLMè°ƒç”¨
   - æ‰¹å¤„ç†ä¼˜åŒ–embeddingç”Ÿæˆ
   - å¼‚æ­¥å¤„ç†æå‡å“åº”é€Ÿåº¦
3. **ç”Ÿäº§éƒ¨ç½²**

   - å®ç°ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
   - A/Bæµ‹è¯•ä¸åŒRAGæ–¹æ³•
   - å»ºç«‹åé¦ˆå¾ªç¯æœºåˆ¶

### âš™ï¸ å‚æ•°è°ƒä¼˜æŒ‡å—

#### æ ¸å¿ƒå‚æ•°è¯´æ˜

| å‚æ•°                    | æ¨èèŒƒå›´ | å½±å“                     | è°ƒä¼˜å»ºè®®                          |
| ----------------------- | -------- | ------------------------ | --------------------------------- |
| **chunk_size**    | 200-1000 | ä¸Šä¸‹æ–‡å®Œæ•´æ€§ vs æ£€ç´¢ç²¾åº¦ | æŠ€æœ¯æ–‡æ¡£200-500ï¼Œå™äº‹æ–‡æœ¬500-1000 |
| **chunk_overlap** | 50-200   | è¾¹ç•Œä¿¡æ¯å®Œæ•´æ€§           | ä¸€èˆ¬è®¾ä¸ºchunk_sizeçš„20-30%        |
| **top_k**         | 3-10     | æ£€ç´¢å¬å› vs å™ªå£°         | å¼€å§‹ç”¨5ï¼Œæ ¹æ®å‡†ç¡®ç‡è°ƒæ•´           |
| **temperature**   | 0.0-0.7  | ç”Ÿæˆç¨³å®šæ€§ vs åˆ›é€ æ€§     | äº‹å®æ€§é—®ç­”ç”¨0.0-0.3               |
| **rerank_top_n**  | 2-5      | é‡æ’åºç²¾åº¦               | é€šå¸¸ä¸ºtop_kçš„1/2                  |

#### åˆ†å—ç­–ç•¥é€‰æ‹©

```python
# 1. æŠ€æœ¯æ–‡æ¡£/ä»£ç ï¼šå°chunk + é«˜overlap
chunk_size = 300
overlap = 100

# 2. æ–°é—»/æ–‡ç« ï¼šä¸­ç­‰chunk + ä¸­ç­‰overlap
chunk_size = 500
overlap = 100

# 3. ä¹¦ç±/é•¿æ–‡æœ¬ï¼šå¤§chunk + ä½overlap
chunk_size = 800
overlap = 150

# 4. FAQ/çŸ­æ–‡æœ¬ï¼šå°chunk + æ— overlap
chunk_size = 200
overlap = 0
```

### ğŸ› å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### é—®é¢˜1ï¼šæ£€ç´¢ç»“æœä¸ç›¸å…³

**å¯èƒ½åŸå› **ï¼š

- Embeddingæ¨¡å‹ä¸é€‚åˆé¢†åŸŸ
- chunk_sizeè®¾ç½®ä¸å½“
- top_kè¿‡å¤§å¼•å…¥å™ªå£°

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. ä½¿ç”¨é¢†åŸŸç‰¹å®šçš„Embeddingæ¨¡å‹
2. è°ƒæ•´chunk_sizeï¼ˆå‡å°é€šå¸¸èƒ½æå‡ç²¾åº¦ï¼‰
3. å‡å°top_kæˆ–æ·»åŠ é‡æ’åº
4. å°è¯•HyDERAGæˆ–QueryTransformRAGä¼˜åŒ–æŸ¥è¯¢

#### é—®é¢˜2ï¼šç­”æ¡ˆæˆªæ–­æˆ–ä¸å®Œæ•´

**å¯èƒ½åŸå› **ï¼š

- chunkåœ¨å…³é”®ä¿¡æ¯å¤„åˆ‡æ–­
- æ£€ç´¢çš„chunkä¸åŒ…å«å®Œæ•´ç­”æ¡ˆ

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. å¢åŠ chunk_overlap
2. ä½¿ç”¨ContextEnrichedRAGæ‰©å±•ä¸Šä¸‹æ–‡
3. ä½¿ç”¨SemanticRagè¯­ä¹‰åˆ†å—
4. ä½¿ç”¨RSERAGè·å–è¿ç»­æ®µè½

#### é—®é¢˜3ï¼šå“åº”é€Ÿåº¦æ…¢

**å¯èƒ½åŸå› **ï¼š

- å‘é‡æ£€ç´¢è€—æ—¶
- LLMç”Ÿæˆè€—æ—¶
- å¤šæ¬¡LLMè°ƒç”¨ï¼ˆSelfRAG, CRAGï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. ä½¿ç”¨HierarchyRAGå‡å°‘æ£€ç´¢èŒƒå›´
2. å®ç°æŸ¥è¯¢ç¼“å­˜
3. æ‰¹å¤„ç†embeddingç”Ÿæˆ
4. ä½¿ç”¨æ›´å¿«çš„LLMæ¨¡å‹
5. å¼‚æ­¥å¤„ç†éå…³é”®è·¯å¾„

#### é—®é¢˜4ï¼šæˆæœ¬è¿‡é«˜

**å¯èƒ½åŸå› **ï¼š

- é¢‘ç¹çš„LLMè°ƒç”¨
- å¤§é‡çš„embeddingç”Ÿæˆ
- ä½¿ç”¨æ˜‚è´µçš„æ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. å®ç°å¤šçº§ç¼“å­˜ï¼ˆæŸ¥è¯¢ç¼“å­˜ã€embeddingç¼“å­˜ï¼‰
2. ä½¿ç”¨ContextualCompressionRAGå‡å°‘token
3. æ‰¹å¤„ç†ä¼˜åŒ–APIè°ƒç”¨
4. é€‰æ‹©æ€§ä½¿ç”¨é«˜çº§æ–¹æ³•ï¼ˆä»…å¯¹é‡è¦æŸ¥è¯¢ä½¿ç”¨SelfRAGï¼‰
5. è€ƒè™‘å¼€æºæ¨¡å‹

### ğŸ“ˆ è¯„ä¼°ä¸ç›‘æ§

#### å…³é”®æŒ‡æ ‡

1. **æ£€ç´¢è´¨é‡æŒ‡æ ‡**ï¼š

   - Precision@Kï¼šå‰Kä¸ªç»“æœä¸­ç›¸å…³çš„æ¯”ä¾‹
   - Recall@Kï¼šç›¸å…³ç»“æœè¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹
   - MRR (Mean Reciprocal Rank)ï¼šç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„å¹³å‡æ’åå€’æ•°
   - NDCGï¼šè€ƒè™‘æ’åºçš„ç´¯ç§¯å¢ç›Š
2. **ç”Ÿæˆè´¨é‡æŒ‡æ ‡**ï¼š

   - äº‹å®å‡†ç¡®æ€§ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹
   - å®Œæ•´æ€§ï¼šæ˜¯å¦å›ç­”äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢
   - ç›¸å…³æ€§ï¼šæ˜¯å¦åˆ‡é¢˜
   - æµç•…æ€§ï¼šè¯­è¨€æ˜¯å¦è‡ªç„¶
3. **ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡**ï¼š

   - å“åº”å»¶è¿Ÿï¼ˆP50, P95, P99ï¼‰
   - APIè°ƒç”¨æ¬¡æ•°å’Œæˆæœ¬
   - ç¼“å­˜å‘½ä¸­ç‡
   - ç³»ç»Ÿååé‡

#### è¯„ä¼°æ–¹æ³•

```python
# 1. æ„å»ºè¯„ä¼°æ•°æ®é›†
eval_dataset = [
    {
        "query": "Pythonå¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ",
        "ground_truth": "ä½¿ç”¨try-exceptè¯­å¥...",
        "relevant_docs": ["doc_123", "doc_456"]
    },
    # ... æ›´å¤šæ ·æœ¬ï¼ˆå»ºè®®è‡³å°‘100ä¸ªï¼‰
]

# 2. è¿è¡Œè¯„ä¼°
for sample in eval_dataset:
    # æ£€ç´¢è¯„ä¼°
    retrieved_docs = rag.search(sample["query"])
    precision = calculate_precision(retrieved_docs, sample["relevant_docs"])
    recall = calculate_recall(retrieved_docs, sample["relevant_docs"])

    # ç”Ÿæˆè¯„ä¼°
    answer = rag.query(sample["query"])
    factuality = evaluate_factuality(answer, retrieved_docs)
    relevance = evaluate_relevance(answer, sample["query"])
```
